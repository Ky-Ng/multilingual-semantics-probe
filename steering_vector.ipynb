{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1e7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model Qwen/Qwen2.5-1.5B into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HookedTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-27): 28 x TransformerBlock(\n",
       "      (ln1): RMSNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): RMSNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): GroupedQueryAttention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "        (hook_rot_k): HookPoint()\n",
       "        (hook_rot_q): HookPoint()\n",
       "      )\n",
       "      (mlp): GatedMLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_pre_linear): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): RMSNormPre(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-1.5B\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.bfloat16 if (DEVICE == \"cuda\") else torch.float32\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    # hf_model=model,\n",
    "    device=DEVICE,\n",
    "    dtype=DTYPE,\n",
    "    # fold_ln=False,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0338ddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_aggregation_token = {\n",
    "    \"zh_period\": 1773,\n",
    "    \"en_period\": 13,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "826a4222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check tokenization\n",
      "original text:  有一个厨师吃了每个政治家。有很多个厨师。\n",
      "['有一个', '厨师', '吃了', '每个', '政治', '家', '。', '有很多', '个', '厨师', '。']\n",
      "0 104133 有一个\n",
      "1 114396 厨师\n",
      "2 105705 吃了\n",
      "3 103991 每个\n",
      "4 101091 政治\n",
      "5 45629 家\n",
      "6 1773 。\n",
      "7 101194 有很多\n",
      "8 18947 个\n",
      "9 114396 厨师\n",
      "10 1773 。\n"
     ]
    }
   ],
   "source": [
    "print(\"Sanity check tokenization\")\n",
    "# text = \"A shark ate every pirate.\"\n",
    "text = \"有一个厨师吃了每个政治家。有很多个厨师。\"\n",
    "# text = \"有一个厨师吃了每个政治家。\"\n",
    "# text = \"A shark ate every pirate. There was only one shark.\"\n",
    "tokens = model.to_tokens(text)  # [batch, seq]\n",
    "\n",
    "print(\"original text: \", text)\n",
    "print(model.to_str_tokens(text))\n",
    "for i, token in enumerate(tokens.squeeze()):\n",
    "    print(i, token.item(), tokenizer.decode(token.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9a801647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden(\n",
    "    model: HookedTransformer,\n",
    "    # tokens: torch.Tensor,\n",
    "    text: str,\n",
    "    target_token_id: int,\n",
    ") -> torch.Tensor:\n",
    "    \n",
    "    tokens = model.to_tokens(text)  # [batch, seq]\n",
    "    target_idx = [token.item() for token in tokens.squeeze()].index(target_token_id)\n",
    "\n",
    "    logits, cache = model.run_with_cache(tokens)\n",
    "    n_layers = model.cfg.n_layers\n",
    "\n",
    "    resid_pre  = [cache[f\"blocks.{l}.hook_resid_pre\"]  for l in range(n_layers)]\n",
    "    resid_pre  = torch.stack(resid_pre,  dim=0)\n",
    "\n",
    "    hidden = resid_pre[:, :, target_idx, :].squeeze(1) # [layer, d_model]\n",
    "\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1c7c8cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0081, -0.0420, -0.0364,  ..., -0.0140,  0.0491,  0.0125],\n",
       "        [ 0.5295, -0.1512, -0.2002,  ..., -0.1003,  0.2558, -0.1710],\n",
       "        [ 0.5128, -0.0748,  0.0432,  ..., -0.3115,  0.4052, -0.1939],\n",
       "        ...,\n",
       "        [ 3.3788, -1.1827,  0.4156,  ..., -4.4232,  0.5269, -2.2891],\n",
       "        [-0.1353, -1.2426,  0.5987,  ..., -5.0756,  1.7250, -4.4888],\n",
       "        [-0.1339, -0.1696,  0.2441,  ..., -4.2331,  0.0539, -4.4301]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_hidden(model, text, target_aggregation_token[\"zh_period\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
