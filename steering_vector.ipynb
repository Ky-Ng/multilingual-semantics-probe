{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5664270",
   "metadata": {},
   "source": [
    "# Steering Vectors\n",
    "- Find and evalulate candidate steering vectors for inverse/surface scope in doubly quantified sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97eee81",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0e1e7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyleng/py_venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model Qwen/Qwen2.5-1.5B into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HookedTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-27): 28 x TransformerBlock(\n",
       "      (ln1): RMSNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): RMSNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): GroupedQueryAttention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "        (hook_rot_k): HookPoint()\n",
       "        (hook_rot_q): HookPoint()\n",
       "      )\n",
       "      (mlp): GatedMLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_pre_linear): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): RMSNormPre(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-1.5B\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.bfloat16 if (DEVICE == \"cuda\") else torch.float32\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    device=DEVICE,\n",
    "    dtype=DTYPE,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5c3f78",
   "metadata": {},
   "source": [
    "## Extract $h_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0338ddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_aggregation_token = {\n",
    "    \"zh_period\": 1773,\n",
    "    \"en_period\": 13,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "826a4222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check tokenization\n",
      "original text:  有一个厨师吃了每个政治家。有很多个厨师。\n",
      "['有一个', '厨师', '吃了', '每个', '政治', '家', '。', '有很多', '个', '厨师', '。']\n",
      "0 104133 有一个\n",
      "1 114396 厨师\n",
      "2 105705 吃了\n",
      "3 103991 每个\n",
      "4 101091 政治\n",
      "5 45629 家\n",
      "6 1773 。\n",
      "7 101194 有很多\n",
      "8 18947 个\n",
      "9 114396 厨师\n",
      "10 1773 。\n"
     ]
    }
   ],
   "source": [
    "print(\"Sanity check tokenization\")\n",
    "# text = \"A shark ate every pirate.\"\n",
    "text = \"有一个厨师吃了每个政治家。有很多个厨师。\"\n",
    "# text = \"有一个厨师吃了每个政治家。\"\n",
    "# text = \"A shark ate every pirate. There was only one shark.\"\n",
    "tokens = model.to_tokens(text)  # [batch, seq]\n",
    "\n",
    "print(\"original text: \", text)\n",
    "print(model.to_str_tokens(text))\n",
    "for i, token in enumerate(tokens.squeeze()):\n",
    "    print(i, token.item(), tokenizer.decode(token.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a801647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO batch this\n",
    "def get_hidden(\n",
    "    model: HookedTransformer,\n",
    "    # tokens: torch.Tensor,\n",
    "    text: str,\n",
    "    target_token_id: int,\n",
    ") -> torch.Tensor:\n",
    "    \n",
    "    tokens = model.to_tokens(text)  # [batch, seq]\n",
    "    target_idx = [token.item() for token in tokens.squeeze()].index(target_token_id)\n",
    "\n",
    "    logits, cache = model.run_with_cache(tokens)\n",
    "    n_layers = model.cfg.n_layers\n",
    "\n",
    "    resid_pre  = [cache[f\"blocks.{l}.hook_resid_pre\"]  for l in range(n_layers)]\n",
    "    resid_pre  = torch.stack(resid_pre,  dim=0)\n",
    "\n",
    "    hidden = resid_pre[:, :, target_idx, :].squeeze(1) # [layer, d_model]\n",
    "\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d37155e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c7c8cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mu_hidden(df: pd.DataFrame, language: str, scope: str, target_token_id: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns the average activation for a \n",
    "    \"\"\"\n",
    "    activations = []\n",
    "    for sentence in df[df[\"language\"] == language][df[\"preference_mean\"] == scope][\"sentence\"]:\n",
    "        activations.append(get_hidden(model, sentence, target_token_id))\n",
    "\n",
    "    return torch.stack(activations).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd72f9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_PERIOD_ID = 13\n",
    "ZH_PERIOD_ID = 1773\n",
    "\n",
    "def get_boundary_id(lang: str) -> int:\n",
    "    return EN_PERIOD_ID if lang == \"en\" else ZH_PERIOD_ID\n",
    "\n",
    "def prompt_period_index(model, prompt: str, boundary_id: int) -> int:\n",
    "    ids = model.to_tokens(prompt, prepend_bos=False).squeeze(0)\n",
    "    return list(ids).index(boundary_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27eb2401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>stimulus_id</th>\n",
       "      <th>concept_id</th>\n",
       "      <th>language</th>\n",
       "      <th>template_id</th>\n",
       "      <th>subj</th>\n",
       "      <th>obj</th>\n",
       "      <th>verb</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tid</th>\n",
       "      <th>...</th>\n",
       "      <th>continuation_text_surface</th>\n",
       "      <th>full_text_inverse</th>\n",
       "      <th>full_text_surface</th>\n",
       "      <th>delta_sum</th>\n",
       "      <th>ratio_sum</th>\n",
       "      <th>delta_mean</th>\n",
       "      <th>ratio_mean</th>\n",
       "      <th>preference_sum</th>\n",
       "      <th>preference_mean</th>\n",
       "      <th>t_prompt_period</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Qwen_Qwen2.5-1.5B</td>\n",
       "      <td>en-t0_s0_o0_v0</td>\n",
       "      <td>shark|pirate|ate</td>\n",
       "      <td>en</td>\n",
       "      <td>en_0</td>\n",
       "      <td>shark</td>\n",
       "      <td>pirate</td>\n",
       "      <td>ate</td>\n",
       "      <td>Every shark ate a pirate.</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>There was only one pirate.</td>\n",
       "      <td>Every shark ate a pirate. There were many pira...</td>\n",
       "      <td>Every shark ate a pirate. There was only one p...</td>\n",
       "      <td>1.534791</td>\n",
       "      <td>4.640356</td>\n",
       "      <td>-0.157645</td>\n",
       "      <td>0.854153</td>\n",
       "      <td>inverse</td>\n",
       "      <td>surface</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen_Qwen2.5-1.5B</td>\n",
       "      <td>en-t0_s0_o0_v1</td>\n",
       "      <td>shark|pirate|helped</td>\n",
       "      <td>en</td>\n",
       "      <td>en_0</td>\n",
       "      <td>shark</td>\n",
       "      <td>pirate</td>\n",
       "      <td>helped</td>\n",
       "      <td>Every shark helped a pirate.</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>There was only one pirate.</td>\n",
       "      <td>Every shark helped a pirate. There were many p...</td>\n",
       "      <td>Every shark helped a pirate. There was only on...</td>\n",
       "      <td>1.788671</td>\n",
       "      <td>5.981495</td>\n",
       "      <td>-0.079300</td>\n",
       "      <td>0.923763</td>\n",
       "      <td>inverse</td>\n",
       "      <td>surface</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Qwen_Qwen2.5-1.5B</td>\n",
       "      <td>en-t0_s0_o0_v2</td>\n",
       "      <td>shark|pirate|pushed</td>\n",
       "      <td>en</td>\n",
       "      <td>en_0</td>\n",
       "      <td>shark</td>\n",
       "      <td>pirate</td>\n",
       "      <td>pushed</td>\n",
       "      <td>Every shark pushed a pirate.</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>There was only one pirate.</td>\n",
       "      <td>Every shark pushed a pirate. There were many p...</td>\n",
       "      <td>Every shark pushed a pirate. There was only on...</td>\n",
       "      <td>1.597416</td>\n",
       "      <td>4.940250</td>\n",
       "      <td>-0.122437</td>\n",
       "      <td>0.884762</td>\n",
       "      <td>inverse</td>\n",
       "      <td>surface</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Qwen_Qwen2.5-1.5B</td>\n",
       "      <td>en-t0_s0_o0_v3</td>\n",
       "      <td>shark|pirate|chased</td>\n",
       "      <td>en</td>\n",
       "      <td>en_0</td>\n",
       "      <td>shark</td>\n",
       "      <td>pirate</td>\n",
       "      <td>chased</td>\n",
       "      <td>Every shark chased a pirate.</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>There was only one pirate.</td>\n",
       "      <td>Every shark chased a pirate. There were many p...</td>\n",
       "      <td>Every shark chased a pirate. There was only on...</td>\n",
       "      <td>-0.460211</td>\n",
       "      <td>0.631151</td>\n",
       "      <td>-0.496891</td>\n",
       "      <td>0.608419</td>\n",
       "      <td>surface</td>\n",
       "      <td>surface</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Qwen_Qwen2.5-1.5B</td>\n",
       "      <td>en-t0_s0_o1_v0</td>\n",
       "      <td>shark|student|ate</td>\n",
       "      <td>en</td>\n",
       "      <td>en_0</td>\n",
       "      <td>shark</td>\n",
       "      <td>student</td>\n",
       "      <td>ate</td>\n",
       "      <td>Every shark ate a student.</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>There was only one student.</td>\n",
       "      <td>Every shark ate a student. There were many stu...</td>\n",
       "      <td>Every shark ate a student. There was only one ...</td>\n",
       "      <td>2.499035</td>\n",
       "      <td>12.170742</td>\n",
       "      <td>0.010185</td>\n",
       "      <td>1.010237</td>\n",
       "      <td>inverse</td>\n",
       "      <td>inverse</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Qwen_Qwen2.5-1.5B</td>\n",
       "      <td>zh-t0_s4_o3_v3</td>\n",
       "      <td>袋鼠|游客|追</td>\n",
       "      <td>zh</td>\n",
       "      <td>zh_0</td>\n",
       "      <td>袋鼠</td>\n",
       "      <td>游客</td>\n",
       "      <td>追</td>\n",
       "      <td>每个只袋鼠追了一个游客。</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>只有一只游客。</td>\n",
       "      <td>每个只袋鼠追了一个游客。有很多只游客。</td>\n",
       "      <td>每个只袋鼠追了一个游客。只有一只游客。</td>\n",
       "      <td>2.758894</td>\n",
       "      <td>15.782377</td>\n",
       "      <td>-0.447330</td>\n",
       "      <td>0.639333</td>\n",
       "      <td>inverse</td>\n",
       "      <td>surface</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Qwen_Qwen2.5-1.5B</td>\n",
       "      <td>zh-t0_s4_o4_v0</td>\n",
       "      <td>袋鼠|政治家|吃</td>\n",
       "      <td>zh</td>\n",
       "      <td>zh_0</td>\n",
       "      <td>袋鼠</td>\n",
       "      <td>政治家</td>\n",
       "      <td>吃</td>\n",
       "      <td>每个只袋鼠吃了一个政治家。</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>只有一只政治家。</td>\n",
       "      <td>每个只袋鼠吃了一个政治家。有很多只政治家。</td>\n",
       "      <td>每个只袋鼠吃了一个政治家。只有一只政治家。</td>\n",
       "      <td>3.729450</td>\n",
       "      <td>41.656200</td>\n",
       "      <td>0.090819</td>\n",
       "      <td>1.095070</td>\n",
       "      <td>inverse</td>\n",
       "      <td>inverse</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>Qwen_Qwen2.5-1.5B</td>\n",
       "      <td>zh-t0_s4_o4_v1</td>\n",
       "      <td>袋鼠|政治家|帮助</td>\n",
       "      <td>zh</td>\n",
       "      <td>zh_0</td>\n",
       "      <td>袋鼠</td>\n",
       "      <td>政治家</td>\n",
       "      <td>帮助</td>\n",
       "      <td>每个只袋鼠帮助了一个政治家。</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>只有一只政治家。</td>\n",
       "      <td>每个只袋鼠帮助了一个政治家。有很多只政治家。</td>\n",
       "      <td>每个只袋鼠帮助了一个政治家。只有一只政治家。</td>\n",
       "      <td>4.993790</td>\n",
       "      <td>147.494321</td>\n",
       "      <td>0.214477</td>\n",
       "      <td>1.239213</td>\n",
       "      <td>inverse</td>\n",
       "      <td>inverse</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Qwen_Qwen2.5-1.5B</td>\n",
       "      <td>zh-t0_s4_o4_v2</td>\n",
       "      <td>袋鼠|政治家|推</td>\n",
       "      <td>zh</td>\n",
       "      <td>zh_0</td>\n",
       "      <td>袋鼠</td>\n",
       "      <td>政治家</td>\n",
       "      <td>推</td>\n",
       "      <td>每个只袋鼠推了一个政治家。</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>只有一只政治家。</td>\n",
       "      <td>每个只袋鼠推了一个政治家。有很多只政治家。</td>\n",
       "      <td>每个只袋鼠推了一个政治家。只有一只政治家。</td>\n",
       "      <td>4.553782</td>\n",
       "      <td>94.990939</td>\n",
       "      <td>0.186490</td>\n",
       "      <td>1.205012</td>\n",
       "      <td>inverse</td>\n",
       "      <td>inverse</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Qwen_Qwen2.5-1.5B</td>\n",
       "      <td>zh-t0_s4_o4_v3</td>\n",
       "      <td>袋鼠|政治家|追</td>\n",
       "      <td>zh</td>\n",
       "      <td>zh_0</td>\n",
       "      <td>袋鼠</td>\n",
       "      <td>政治家</td>\n",
       "      <td>追</td>\n",
       "      <td>每个只袋鼠追了一个政治家。</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>只有一只政治家。</td>\n",
       "      <td>每个只袋鼠追了一个政治家。有很多只政治家。</td>\n",
       "      <td>每个只袋鼠追了一个政治家。只有一只政治家。</td>\n",
       "      <td>2.476442</td>\n",
       "      <td>11.898857</td>\n",
       "      <td>-0.134735</td>\n",
       "      <td>0.873948</td>\n",
       "      <td>inverse</td>\n",
       "      <td>surface</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model     stimulus_id           concept_id language  \\\n",
       "0    Qwen_Qwen2.5-1.5B  en-t0_s0_o0_v0     shark|pirate|ate       en   \n",
       "1    Qwen_Qwen2.5-1.5B  en-t0_s0_o0_v1  shark|pirate|helped       en   \n",
       "2    Qwen_Qwen2.5-1.5B  en-t0_s0_o0_v2  shark|pirate|pushed       en   \n",
       "3    Qwen_Qwen2.5-1.5B  en-t0_s0_o0_v3  shark|pirate|chased       en   \n",
       "4    Qwen_Qwen2.5-1.5B  en-t0_s0_o1_v0    shark|student|ate       en   \n",
       "..                 ...             ...                  ...      ...   \n",
       "195  Qwen_Qwen2.5-1.5B  zh-t0_s4_o3_v3              袋鼠|游客|追       zh   \n",
       "196  Qwen_Qwen2.5-1.5B  zh-t0_s4_o4_v0             袋鼠|政治家|吃       zh   \n",
       "197  Qwen_Qwen2.5-1.5B  zh-t0_s4_o4_v1            袋鼠|政治家|帮助       zh   \n",
       "198  Qwen_Qwen2.5-1.5B  zh-t0_s4_o4_v2             袋鼠|政治家|推       zh   \n",
       "199  Qwen_Qwen2.5-1.5B  zh-t0_s4_o4_v3             袋鼠|政治家|追       zh   \n",
       "\n",
       "    template_id   subj      obj    verb                      sentence  tid  \\\n",
       "0          en_0  shark   pirate     ate     Every shark ate a pirate.    0   \n",
       "1          en_0  shark   pirate  helped  Every shark helped a pirate.    0   \n",
       "2          en_0  shark   pirate  pushed  Every shark pushed a pirate.    0   \n",
       "3          en_0  shark   pirate  chased  Every shark chased a pirate.    0   \n",
       "4          en_0  shark  student     ate    Every shark ate a student.    0   \n",
       "..          ...    ...      ...     ...                           ...  ...   \n",
       "195        zh_0     袋鼠       游客       追                  每个只袋鼠追了一个游客。    0   \n",
       "196        zh_0     袋鼠      政治家       吃                 每个只袋鼠吃了一个政治家。    0   \n",
       "197        zh_0     袋鼠      政治家      帮助                每个只袋鼠帮助了一个政治家。    0   \n",
       "198        zh_0     袋鼠      政治家       推                 每个只袋鼠推了一个政治家。    0   \n",
       "199        zh_0     袋鼠      政治家       追                 每个只袋鼠追了一个政治家。    0   \n",
       "\n",
       "     ...     continuation_text_surface  \\\n",
       "0    ...    There was only one pirate.   \n",
       "1    ...    There was only one pirate.   \n",
       "2    ...    There was only one pirate.   \n",
       "3    ...    There was only one pirate.   \n",
       "4    ...   There was only one student.   \n",
       "..   ...                           ...   \n",
       "195  ...                       只有一只游客。   \n",
       "196  ...                      只有一只政治家。   \n",
       "197  ...                      只有一只政治家。   \n",
       "198  ...                      只有一只政治家。   \n",
       "199  ...                      只有一只政治家。   \n",
       "\n",
       "                                     full_text_inverse  \\\n",
       "0    Every shark ate a pirate. There were many pira...   \n",
       "1    Every shark helped a pirate. There were many p...   \n",
       "2    Every shark pushed a pirate. There were many p...   \n",
       "3    Every shark chased a pirate. There were many p...   \n",
       "4    Every shark ate a student. There were many stu...   \n",
       "..                                                 ...   \n",
       "195                                每个只袋鼠追了一个游客。有很多只游客。   \n",
       "196                              每个只袋鼠吃了一个政治家。有很多只政治家。   \n",
       "197                             每个只袋鼠帮助了一个政治家。有很多只政治家。   \n",
       "198                              每个只袋鼠推了一个政治家。有很多只政治家。   \n",
       "199                              每个只袋鼠追了一个政治家。有很多只政治家。   \n",
       "\n",
       "                                     full_text_surface delta_sum   ratio_sum  \\\n",
       "0    Every shark ate a pirate. There was only one p...  1.534791    4.640356   \n",
       "1    Every shark helped a pirate. There was only on...  1.788671    5.981495   \n",
       "2    Every shark pushed a pirate. There was only on...  1.597416    4.940250   \n",
       "3    Every shark chased a pirate. There was only on... -0.460211    0.631151   \n",
       "4    Every shark ate a student. There was only one ...  2.499035   12.170742   \n",
       "..                                                 ...       ...         ...   \n",
       "195                                每个只袋鼠追了一个游客。只有一只游客。  2.758894   15.782377   \n",
       "196                              每个只袋鼠吃了一个政治家。只有一只政治家。  3.729450   41.656200   \n",
       "197                             每个只袋鼠帮助了一个政治家。只有一只政治家。  4.993790  147.494321   \n",
       "198                              每个只袋鼠推了一个政治家。只有一只政治家。  4.553782   94.990939   \n",
       "199                              每个只袋鼠追了一个政治家。只有一只政治家。  2.476442   11.898857   \n",
       "\n",
       "     delta_mean  ratio_mean  preference_sum  preference_mean  t_prompt_period  \n",
       "0     -0.157645    0.854153         inverse          surface                5  \n",
       "1     -0.079300    0.923763         inverse          surface                5  \n",
       "2     -0.122437    0.884762         inverse          surface                5  \n",
       "3     -0.496891    0.608419         surface          surface                5  \n",
       "4      0.010185    1.010237         inverse          inverse                5  \n",
       "..          ...         ...             ...              ...              ...  \n",
       "195   -0.447330    0.639333         inverse          surface                7  \n",
       "196    0.090819    1.095070         inverse          inverse                8  \n",
       "197    0.214477    1.239213         inverse          inverse                8  \n",
       "198    0.186490    1.205012         inverse          inverse                8  \n",
       "199   -0.134735    0.873948         inverse          surface                8  \n",
       "\n",
       "[200 rows x 31 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"results/qwen2.5/tokenize_no_space_1.5b/scored_wide_ue_Qwen_Qwen2.5-1.5B-2.jsonl\"\n",
    "df = pd.read_json(path, lines=True)\n",
    "\n",
    "# IMPORTANT: align rows with activations\n",
    "df = df.reset_index(drop=True)\n",
    "df[\"t_prompt_period\"] = df.apply(\n",
    "    lambda r: prompt_period_index(\n",
    "        model,\n",
    "        r[\"sentence\"],\n",
    "        get_boundary_id(r[\"language\"]),\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0f43d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def extract_H_one_layer(\n",
    "    model,\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    layer: int,\n",
    "    hook_point: str = \"resid_pre\",\n",
    "    batch_size: int = 16,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns H: [N, d_model] for the specified layer.\n",
    "    \"\"\"\n",
    "    prompts = df[\"sentence\"].tolist()\n",
    "    t_pos = torch.tensor(df[\"t_prompt_period\"].tolist(), dtype=torch.long)\n",
    "\n",
    "    outs = []\n",
    "\n",
    "    for start in range(0, len(prompts), batch_size):\n",
    "        end = min(len(prompts), start + batch_size)\n",
    "\n",
    "        p_batch = prompts[start:end]\n",
    "        t_batch = t_pos[start:end]\n",
    "\n",
    "        toks = model.to_tokens(p_batch)\n",
    "        hook = f\"blocks.{layer}.hook_{hook_point}\"\n",
    "\n",
    "        _, cache = model.run_with_cache(toks, names_filter=[hook])\n",
    "\n",
    "        acts = cache[hook]              # [B, T, d_model]\n",
    "        B = acts.shape[0]\n",
    "        b_idx = torch.arange(B, device=acts.device)\n",
    "\n",
    "        H_batch = acts[b_idx, t_batch.to(acts.device)]\n",
    "        outs.append(H_batch.detach().cpu())\n",
    "\n",
    "    return torch.cat(outs, dim=0)       # [B, d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58470b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mu_from_H(\n",
    "    H: torch.Tensor,      # [B, d_model]\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    language: str,\n",
    "    scope: str,           # \"surface\" | \"inverse\"\n",
    ") -> torch.Tensor:\n",
    "    mask = (\n",
    "        (df[\"language\"] == language) &\n",
    "        (df[\"preference_mean\"] == scope)\n",
    "    ).to_numpy()\n",
    "\n",
    "    if  np.all(mask == False):\n",
    "        _, d_model = H.shape\n",
    "        return torch.zeros(d_model)\n",
    "    return H[mask].mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31f1da6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_v_hat(H_train, df_train, *, language: str):\n",
    "    mu_surface = get_mu_from_H(H_train, df_train, language=language, scope=\"surface\")\n",
    "    mu_inverse = get_mu_from_H(H_train, df_train, language=language, scope=\"inverse\")\n",
    "    v = mu_inverse - mu_surface\n",
    "    return v / (v.norm() + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c7665a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_separability_hist(\n",
    "    H,\n",
    "    df,\n",
    "    v_hat,\n",
    "    *,\n",
    "    language: str,\n",
    "    title: str,\n",
    "    save_path: str | None = None,\n",
    "):\n",
    "    mask = (df[\"language\"] == language).to_numpy()\n",
    "    H_lang = H[mask]\n",
    "    df_lang = df[mask]\n",
    "\n",
    "    p = (H_lang @ v_hat.cpu()).numpy()\n",
    "    y = df_lang[\"preference_mean\"].to_numpy()\n",
    "\n",
    "    p_surf = p[y == \"surface\"]\n",
    "    p_inv  = p[y == \"inverse\"]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    ax.hist(p_surf, bins=40, alpha=0.6, label=\"surface\")\n",
    "    ax.hist(p_inv,  bins=40, alpha=0.6, label=\"inverse\")\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"p = vᵀh\")\n",
    "    ax.set_ylabel(\"count\")\n",
    "    ax.legend()\n",
    "\n",
    "    if save_path is not None:\n",
    "        fig.savefig(save_path, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8b42d18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts:\n",
      "language  preference_mean\n",
      "en        inverse             9\n",
      "          surface            91\n",
      "zh        inverse            48\n",
      "          surface            52\n",
      "dtype: int64\n",
      "Extracting layer  0\n",
      "||v_en|| = 0.0\n",
      "||v_zh|| = 0.0\n",
      "cross-linguistic alignment tensor(0.)\n",
      "Extracting layer  1\n",
      "||v_en|| = 1.0000001192092896\n",
      "||v_zh|| = 0.9999999403953552\n",
      "cross-linguistic alignment tensor(0.0150)\n",
      "Extracting layer  2\n",
      "||v_en|| = 1.0\n",
      "||v_zh|| = 1.0\n",
      "cross-linguistic alignment tensor(0.0393)\n",
      "Extracting layer  3\n",
      "||v_en|| = 0.9999999403953552\n",
      "||v_zh|| = 1.0000001192092896\n",
      "cross-linguistic alignment tensor(0.1365)\n",
      "Extracting layer  4\n",
      "||v_en|| = 1.0\n",
      "||v_zh|| = 0.9999998807907104\n",
      "cross-linguistic alignment tensor(0.1588)\n",
      "Extracting layer  5\n",
      "||v_en|| = 1.0\n",
      "||v_zh|| = 1.0\n",
      "cross-linguistic alignment tensor(0.1462)\n",
      "Extracting layer  6\n",
      "||v_en|| = 1.0000001192092896\n",
      "||v_zh|| = 1.0\n",
      "cross-linguistic alignment tensor(-0.0535)\n",
      "Extracting layer  7\n",
      "||v_en|| = 0.9999999403953552\n",
      "||v_zh|| = 1.0000001192092896\n",
      "cross-linguistic alignment tensor(-0.0231)\n",
      "Extracting layer  8\n",
      "||v_en|| = 0.9999999403953552\n",
      "||v_zh|| = 1.0\n",
      "cross-linguistic alignment tensor(-0.0295)\n",
      "Extracting layer  9\n",
      "||v_en|| = 1.0\n",
      "||v_zh|| = 0.9999998211860657\n",
      "cross-linguistic alignment tensor(-0.0393)\n",
      "Extracting layer  10\n",
      "||v_en|| = 1.0\n",
      "||v_zh|| = 1.0\n",
      "cross-linguistic alignment tensor(0.0109)\n",
      "Extracting layer  11\n",
      "||v_en|| = 1.0\n",
      "||v_zh|| = 1.0000001192092896\n",
      "cross-linguistic alignment tensor(0.0647)\n",
      "Extracting layer  12\n",
      "||v_en|| = 1.0\n",
      "||v_zh|| = 1.0\n",
      "cross-linguistic alignment tensor(0.0140)\n",
      "Extracting layer  13\n",
      "||v_en|| = 1.0000001192092896\n",
      "||v_zh|| = 0.9999999403953552\n",
      "cross-linguistic alignment tensor(0.0319)\n",
      "Extracting layer  14\n",
      "||v_en|| = 0.9999999403953552\n",
      "||v_zh|| = 0.9999998211860657\n",
      "cross-linguistic alignment tensor(0.0661)\n",
      "Extracting layer  15\n",
      "||v_en|| = 0.9999998807907104\n",
      "||v_zh|| = 1.0\n",
      "cross-linguistic alignment tensor(0.0970)\n",
      "Extracting layer  16\n",
      "||v_en|| = 1.0000001192092896\n",
      "||v_zh|| = 0.9999999403953552\n",
      "cross-linguistic alignment tensor(0.0779)\n",
      "Extracting layer  17\n",
      "||v_en|| = 0.9999998211860657\n",
      "||v_zh|| = 0.9999999403953552\n",
      "cross-linguistic alignment tensor(0.1519)\n",
      "Extracting layer  18\n",
      "||v_en|| = 0.9999998211860657\n",
      "||v_zh|| = 1.0000001192092896\n",
      "cross-linguistic alignment tensor(0.0856)\n",
      "Extracting layer  19\n",
      "||v_en|| = 1.0000001192092896\n",
      "||v_zh|| = 0.9999998807907104\n",
      "cross-linguistic alignment tensor(0.1553)\n",
      "Extracting layer  20\n",
      "||v_en|| = 1.0\n",
      "||v_zh|| = 0.9999999403953552\n",
      "cross-linguistic alignment tensor(0.1215)\n",
      "Extracting layer  21\n",
      "||v_en|| = 0.9999998211860657\n",
      "||v_zh|| = 1.0\n",
      "cross-linguistic alignment tensor(0.1897)\n",
      "Extracting layer  22\n",
      "||v_en|| = 1.0\n",
      "||v_zh|| = 1.0\n",
      "cross-linguistic alignment tensor(0.1722)\n",
      "Extracting layer  23\n",
      "||v_en|| = 0.9999999403953552\n",
      "||v_zh|| = 0.9999999403953552\n",
      "cross-linguistic alignment tensor(0.1786)\n",
      "Extracting layer  24\n",
      "||v_en|| = 1.0000001192092896\n",
      "||v_zh|| = 1.0000001192092896\n",
      "cross-linguistic alignment tensor(0.1411)\n",
      "Extracting layer  25\n",
      "||v_en|| = 0.9999998807907104\n",
      "||v_zh|| = 1.0000001192092896\n",
      "cross-linguistic alignment tensor(0.1003)\n",
      "Extracting layer  26\n",
      "||v_en|| = 1.0\n",
      "||v_zh|| = 1.000000238418579\n",
      "cross-linguistic alignment tensor(0.0707)\n",
      "Extracting layer  27\n",
      "||v_en|| = 0.9999999403953552\n",
      "||v_zh|| = 0.9999999403953552\n",
      "cross-linguistic alignment tensor(0.0166)\n"
     ]
    }
   ],
   "source": [
    "print(\"counts:\")\n",
    "print(df.groupby([\"language\", \"preference_mean\"]).size())\n",
    "\n",
    "n_layers = model.cfg.n_layers\n",
    "for layer in range(n_layers):\n",
    "# for layer in [10, 11]:\n",
    "    print(\"Extracting layer \", layer)\n",
    "    H = extract_H_one_layer(model, df, layer=layer)\n",
    "    v_en = fit_v_hat(H, df, language=\"en\")\n",
    "    v_zh = fit_v_hat(H, df, language=\"zh\")\n",
    "\n",
    "    # sanity check\n",
    "    print(\"||v_en|| =\", v_en.norm().item())\n",
    "    print(\"||v_zh|| =\", v_zh.norm().item())\n",
    "\n",
    "    print(\"cross-linguistic alignment\", torch.dot(v_en, v_zh))\n",
    "    save_root = Path(f\"results/plots/{MODEL_NAME}\")\n",
    "    save_root.mkdir(parents=True, exist_ok=True)\n",
    "    plot_separability_hist(H, df, v_en, language=\"en\", title=f\"Layer {layer}: v_en separation for EN data\", save_path=save_root / f\"en/layer_{layer:02d}_en_hist.png\")\n",
    "    plot_separability_hist(H, df, v_zh, language=\"zh\", title=f\"Layer {layer}: v_zh separation for ZH data\", save_path=save_root / f\"zh/layer_{layer:02d}_zh_hist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f3aa98b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_en = fit_v_hat(H, df, language=\"en\")\n",
    "v_zh = fit_v_hat(H, df, language=\"zh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bc597962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts:\n",
      "language  preference_mean\n",
      "en        inverse             9\n",
      "          surface            91\n",
      "zh        inverse            48\n",
      "          surface            52\n",
      "dtype: int64\n",
      "||v_en|| = 0.9999999403953552\n",
      "||v_zh|| = 0.9999999403953552\n"
     ]
    }
   ],
   "source": [
    "print(\"counts:\")\n",
    "print(df.groupby([\"language\", \"preference_mean\"]).size())\n",
    "print(\"||v_en|| =\", v_en.norm().item())\n",
    "print(\"||v_zh|| =\", v_zh.norm().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "508f6606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross-linguistic alignment tensor(0.0166)\n"
     ]
    }
   ],
   "source": [
    "print(\"cross-linguistic alignment\", torch.dot(v_en, v_zh))\n",
    "plot_separability_hist(H, df, v_en, language=\"en\", title=\"EN: v_en on EN\")\n",
    "plot_separability_hist(H, df, v_zh, language=\"zh\", title=\"ZH: v_zh on ZH\")\n",
    "\n",
    "# plot_separability_hist(H, df, v_zh, language=\"en\", title=\"EN: v_zh on EN\") # Should be no clear separation since EN vector already doesn't work\n",
    "# plot_separability_hist(H, df, v_en, language=\"zh\", title=\"ZH: v_en on ZH\") # Dot product was close to 0 so shouldn't show clear separation either"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63b301e",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2dcc08",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
