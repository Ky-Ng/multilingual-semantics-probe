{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61b6f226",
   "metadata": {},
   "source": [
    "# Multilingual Semantics Probe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ea80eb",
   "metadata": {},
   "source": [
    "## Step 1: Corpus Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c9961ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import itertools\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1c6656ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "STIMULI_DIR = \"./stimuli\"\n",
    "\n",
    "if not os.path.exists(STIMULI_DIR):\n",
    "    os.mkdir(STIMULI_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e5aba402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- English lexicon ---\n",
    "EN_SUBJECTS = [\n",
    "    \"shark\",\n",
    "    \"robot\",\n",
    "    \"chef\",\n",
    "    \"dog\",\n",
    "]\n",
    "\n",
    "EN_OBJECTS = [\n",
    "    \"pirate\",\n",
    "    \"student\",\n",
    "    \"doctor\",\n",
    "    \"tourist\",\n",
    "]\n",
    "\n",
    "# Use correct simple past forms\n",
    "EN_VERBS_PAST = [\n",
    "    \"ate\",\n",
    "    \"helped\",\n",
    "    \"pushed\",\n",
    "    \"chased\",\n",
    "]\n",
    "\n",
    "# --- Mandarin lexicon ---\n",
    "# Bare nouns only (no quantifiers inside)\n",
    "ZH_SUBJECTS = [\n",
    "    \"鲨鱼\",\n",
    "    \"机器人\",\n",
    "    \"厨师\",\n",
    "    \"狗\",\n",
    "]\n",
    "\n",
    "ZH_OBJECTS = [\n",
    "    \"海盗\",\n",
    "    \"学生\",\n",
    "    \"医生\",\n",
    "    \"游客\",\n",
    "]\n",
    "\n",
    "# Verb stems compatible with 了\n",
    "ZH_VERBS = [\n",
    "    \"吃\",\n",
    "    \"帮助\",\n",
    "    \"推\",\n",
    "    \"追\",\n",
    "]\n",
    "\n",
    "# Optional classifier map (defaults to 个)\n",
    "ZH_CLASSIFIER: Dict[str, str] = {\n",
    "    \"鲨鱼\": \"只\",\n",
    "    \"狗\": \"只\",\n",
    "    \"机器人\": \"个\",\n",
    "    \"厨师\": \"个\",\n",
    "    \"海盗\": \"个\",\n",
    "    \"学生\": \"个\",\n",
    "    \"医生\": \"个\",\n",
    "    \"游客\": \"个\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "44a60565",
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_TEMPLATES = [\n",
    "    # Classic ambiguous English form\n",
    "    \"A {subj} {verb_past} every {obj}.\",\n",
    "]\n",
    "\n",
    "ZH_TEMPLATES = [\n",
    "    # Canonical Mandarin surface-scope reading\n",
    "    \"有一{cl}{subj}{verb}了每个{obj}。\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "2202da3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Stimulus:\n",
    "    language: str\n",
    "    template_id: str\n",
    "    subj: str\n",
    "    obj: str\n",
    "    verb: str\n",
    "    sentence: str\n",
    "\n",
    "\n",
    "def get_classifier(noun: str, cl_map: Dict[str, str]) -> str:\n",
    "    return cl_map.get(noun, \"个\")\n",
    "\n",
    "\n",
    "def generate_english(\n",
    "    subjects: List[str],\n",
    "    objects: List[str],\n",
    "    verbs_past: List[str],\n",
    ") -> List[Stimulus]:\n",
    "    out: List[Stimulus] = []\n",
    "    for tid, tmpl in enumerate(EN_TEMPLATES):\n",
    "        for subj, obj, verb in itertools.product(subjects, objects, verbs_past):\n",
    "            out.append(\n",
    "                Stimulus(\n",
    "                    language=\"en\",\n",
    "                    template_id=f\"en_{tid}\",\n",
    "                    subj=subj,\n",
    "                    obj=obj,\n",
    "                    verb=verb,\n",
    "                    sentence=tmpl.format(\n",
    "                        subj=subj,\n",
    "                        obj=obj,\n",
    "                        verb_past=verb,\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "    return out\n",
    "\n",
    "\n",
    "def generate_mandarin(\n",
    "    subjects: List[str],\n",
    "    objects: List[str],\n",
    "    verbs: List[str],\n",
    "    cl_map: Dict[str, str],\n",
    ") -> List[Stimulus]:\n",
    "    out: List[Stimulus] = []\n",
    "    for tid, tmpl in enumerate(ZH_TEMPLATES):\n",
    "        for subj, obj, verb in itertools.product(subjects, objects, verbs):\n",
    "            cl = get_classifier(subj, cl_map)\n",
    "            out.append(\n",
    "                Stimulus(\n",
    "                    language=\"zh\",\n",
    "                    template_id=f\"zh_{tid}\",\n",
    "                    subj=subj,\n",
    "                    obj=obj,\n",
    "                    verb=verb,\n",
    "                    sentence=tmpl.format(\n",
    "                        cl=cl,\n",
    "                        subj=subj,\n",
    "                        obj=obj,\n",
    "                        verb=verb,\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d6e34c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stimuli = []\n",
    "stimuli += generate_english(EN_SUBJECTS, EN_OBJECTS, EN_VERBS_PAST)\n",
    "stimuli += generate_mandarin(ZH_SUBJECTS, ZH_OBJECTS, ZH_VERBS, ZH_CLASSIFIER)\n",
    "\n",
    "df = pd.DataFrame([s.__dict__ for s in stimuli])\n",
    "\n",
    "# Stable IDs for downstream scoring\n",
    "df.insert(\n",
    "    0,\n",
    "    \"stimulus_id\",\n",
    "    [\n",
    "        f\"{row.language}-{row.template_id}-{row.Index:06d}\"\n",
    "        for row in df.itertuples()\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "186f7040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total stimuli: 128\n",
      "language\n",
      "en    64\n",
      "zh    64\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stimulus_id</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>en-en_0-000045</td>\n",
       "      <td>A chef helped every tourist.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>en-en_0-000029</td>\n",
       "      <td>A robot helped every tourist.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>en-en_0-000043</td>\n",
       "      <td>A chef chased every doctor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>en-en_0-000061</td>\n",
       "      <td>A dog helped every tourist.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>en-en_0-000034</td>\n",
       "      <td>A chef pushed every pirate.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       stimulus_id                       sentence\n",
       "45  en-en_0-000045   A chef helped every tourist.\n",
       "29  en-en_0-000029  A robot helped every tourist.\n",
       "43  en-en_0-000043    A chef chased every doctor.\n",
       "61  en-en_0-000061    A dog helped every tourist.\n",
       "34  en-en_0-000034    A chef pushed every pirate."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stimulus_id</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>zh-zh_0-000109</td>\n",
       "      <td>有一个厨师帮助了每个游客。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>zh-zh_0-000093</td>\n",
       "      <td>有一个机器人帮助了每个游客。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>zh-zh_0-000107</td>\n",
       "      <td>有一个厨师追了每个医生。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>zh-zh_0-000125</td>\n",
       "      <td>有一只狗帮助了每个游客。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>zh-zh_0-000098</td>\n",
       "      <td>有一个厨师推了每个海盗。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        stimulus_id        sentence\n",
       "109  zh-zh_0-000109   有一个厨师帮助了每个游客。\n",
       "93   zh-zh_0-000093  有一个机器人帮助了每个游客。\n",
       "107  zh-zh_0-000107    有一个厨师追了每个医生。\n",
       "125  zh-zh_0-000125    有一只狗帮助了每个游客。\n",
       "98   zh-zh_0-000098    有一个厨师推了每个海盗。"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Total stimuli:\", len(df))\n",
    "print(df[\"language\"].value_counts())\n",
    "\n",
    "display(\n",
    "    df[df[\"language\"] == \"en\"][[\"stimulus_id\", \"sentence\"]].sample(\n",
    "        min(5, (df[\"language\"] == \"en\").sum()),\n",
    "        random_state=0,\n",
    "    )\n",
    ")\n",
    "\n",
    "display(\n",
    "    df[df[\"language\"] == \"zh\"][[\"stimulus_id\", \"sentence\"]].sample(\n",
    "        min(5, (df[\"language\"] == \"zh\").sum()),\n",
    "        random_state=0,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "891de94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote stimuli.csv and stimuli.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Serialize\n",
    "df.to_csv(os.path.join(STIMULI_DIR,\"stimuli.csv\"), index=False)\n",
    "\n",
    "with open(os.path.join(STIMULI_DIR, \"stimuli.jsonl\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    for row in df.to_dict(orient=\"records\"):\n",
    "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Wrote stimuli.csv and stimuli.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79665af",
   "metadata": {},
   "source": [
    "### Add Natural Language Continuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b9bc3d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_CONTINUATIONS = {\n",
    "    \"surface\": \" There was only one {subj}.\",\n",
    "    \"inverse\": \" There were many {subj}.\",\n",
    "}\n",
    "\n",
    "# Mandarin: keep equally short.\n",
    "# Note: plural is usually implicit; \"很多\" is a decent lexical cue.\n",
    "ZH_CONTINUATIONS = {\n",
    "    \"surface\": \" 只有一{cl}{subj}。\",\n",
    "    \"inverse\": \" 有很多{cl}{subj}。\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "98b3708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_continuations(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for r in df.itertuples(index=False):\n",
    "        base = r._asdict()\n",
    "\n",
    "        if base[\"language\"] == \"en\":\n",
    "            # naive pluralization: add \"s\"\n",
    "            # If you care about irregular plurals later, add a map.\n",
    "            subj_plural = base[\"subj\"] + \"s\"\n",
    "            cont_map = {\n",
    "                \"surface\": EN_CONTINUATIONS[\"surface\"].format(subj=base[\"subj\"]),\n",
    "                \"inverse\": EN_CONTINUATIONS[\"inverse\"].format(subj=subj_plural),\n",
    "            }\n",
    "\n",
    "        elif base[\"language\"] == \"zh\":\n",
    "            cl = ZH_CLASSIFIER.get(base[\"subj\"], \"个\")\n",
    "            cont_map = {\n",
    "                \"surface\": ZH_CONTINUATIONS[\"surface\"].format(cl=cl, subj=base[\"subj\"]),\n",
    "                \"inverse\": ZH_CONTINUATIONS[\"inverse\"].format(cl=cl, subj=base[\"subj\"]),\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown language: {base['language']}\")\n",
    "\n",
    "        for cont_type, cont_text in cont_map.items():\n",
    "            ex = dict(base)\n",
    "            ex[\"continuation_type\"] = cont_type            # \"surface\" or \"inverse\"\n",
    "            ex[\"continuation_text\"] = cont_text            # the thing you'll score\n",
    "            ex[\"full_text\"] = base[\"sentence\"] + cont_text # convenient for debugging\n",
    "            rows.append(ex)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "217600e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cont = add_continuations(df)\n",
    "\n",
    "if \"concept_id\" not in df_cont.columns:\n",
    "    concept_series = df_cont[\"subj\"] + \"|\" + df_cont[\"obj\"] + \"|\" + df_cont[\"verb\"]\n",
    "    df_cont.insert(1, \"concept_id\", concept_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d6994fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote stimuli_with_continuations.csv and stimuli_with_continuations.jsonl\n"
     ]
    }
   ],
   "source": [
    "df_cont.to_csv(os.path.join(STIMULI_DIR, \"stimuli_with_continuations.csv\"), index=False)\n",
    "\n",
    "with open(os.path.join(STIMULI_DIR, \"stimuli_with_continuations.jsonl\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    for row in df_cont.to_dict(orient=\"records\"):\n",
    "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Wrote stimuli_with_continuations.csv and stimuli_with_continuations.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7115d99",
   "metadata": {},
   "source": [
    "## Step 2: Evaluate Log Probs of Continuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb7f0a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyleng/py_venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e639abf",
   "metadata": {},
   "source": [
    "### Load in Continuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f139f006",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSONL_PATH = \"stimuli/stimuli_with_continuations.jsonl\"\n",
    "\n",
    "def read_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                yield json.loads(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f29998aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = list(read_jsonl(JSONL_PATH))\n",
    "df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c05e25",
   "metadata": {},
   "source": [
    "### Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a52ea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt2\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.float16 if (DEVICE == \"cuda\") else torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1889b2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d341895",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_CSV = \"logprob_surface_vs_inverse.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b7e0983",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a26a9251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=DTYPE\n",
    ").to(DEVICE)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae5f1d3",
   "metadata": {},
   "source": [
    "### Calculate Prefix Log Prob\n",
    "1) Tokenize Prompt and Continuation\n",
    "\n",
    "2) Calculate Log Probs for prompt + continuation\n",
    "\n",
    "3) Calculate continuation log probs: [a] get log prob of each next token from [0,T-1]; [b] sum log probs\n",
    "- Given a prompt of length T, log_prob[:, :T-1, :] gives the probability of the first [0,T-1] tokens\n",
    "\n",
    "```\n",
    "\n",
    "a shark ate every pirate\n",
    "                    ^ stop here; we already know the probability of this word\n",
    "```\n",
    "\n",
    "```python\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        target_log_probs[b,t] = shifted_log_probs[b, t, target_tokens[b,t]]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ba1b175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_probs(prompts: list[str], continuations: list[str]):\n",
    "    # 1) Tokenize Prompt and Continuation\n",
    "    enc_base_prompts = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "\n",
    "        # Handle different length prompts\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "\n",
    "        # Avoid [EOS]/[BOS] from being inserted\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "\n",
    "    full_prompts = [p + c for p, c in zip(prompts, continuations)]\n",
    "    enc_full_prompts = tokenizer(\n",
    "        full_prompts,\n",
    "        return_tensors=\"pt\",\n",
    "\n",
    "        # Handle different length prompts\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "\n",
    "        # Avoid [EOS]/[BOS] from being inserted\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "\n",
    "    input_ids = enc_full_prompts[\"input_ids\"].to(DEVICE)\n",
    "    attention_mask = enc_full_prompts[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "    # 2) Calculate logProbs for prompt + continuation\n",
    "    out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = out.logits  # [B, T, V]\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)  # [B, T, V]\n",
    "\n",
    "    # 3) Calculate continuation log probs: [a] get log prob of each next token from [0,T-1]; [b] sum log probs\n",
    "    target_tokens = input_ids[:, 1:]  # [B, T-1]\n",
    "    shifted_log_probs = log_probs[:, :-1, :]  # [B, T-1, V]\n",
    "\n",
    "    # Select the logProb for the selected token\n",
    "    target_log_probs = torch.gather(\n",
    "        input=shifted_log_probs,\n",
    "        dim=-1,  # Select the log_prob for the selected prompt token in vocab\n",
    "        index=target_tokens.unsqueeze(-1)  # [B, T-1, 1]\n",
    "    ).squeeze(-1)  # [B, T-1]\n",
    "\n",
    "    base_prompt_lens = enc_base_prompts[\"attention_mask\"].sum(dim=1).tolist()\n",
    "    full_prompt_lens = enc_full_prompts[\"attention_mask\"].sum(dim=1).tolist()\n",
    "\n",
    "    # The logProbs for the continuation live at (inclusive)\n",
    "    # logits[m-1:n-2] -> logits[m-1:L-2] -> target_log_probs[m-1:L-1] since we already removed one token in the shift\n",
    "    B, _ = target_log_probs.shape\n",
    "\n",
    "    cont_log_probs_list = []\n",
    "\n",
    "    for b in range(B):\n",
    "        base_prompt_length = base_prompt_lens[b]\n",
    "        full_prompt_length = full_prompt_lens[b]\n",
    "\n",
    "        cont_log_probs = target_log_probs[b,\n",
    "                                          base_prompt_length-1:full_prompt_length-1]\n",
    "        cont_log_probs_sum = cont_log_probs.sum().item()\n",
    "        cont_log_probs_mean = cont_log_probs.mean().item()\n",
    "        n_cont_tokens = full_prompt_length - base_prompt_length  # Sanity Check\n",
    "\n",
    "        cont_log_probs_list.append(\n",
    "            {\"cont_log_probs_sum\": cont_log_probs_sum,\n",
    "                \"cont_log_probs_mean\": cont_log_probs_mean, \n",
    "                \"n_cont_tokens\": n_cont_tokens\n",
    "            }\n",
    "        )\n",
    "        # debugging\n",
    "        # print(base_prompt_length, full_prompt_length)\n",
    "        # print(attention_mask[b].shape, attention_mask[b])\n",
    "        # print(target_log_probs[b].shape, target_log_probs[b])\n",
    "        # print(cont_log_probs.shape,cont_log_probs)\n",
    "        # print(\"=\"*10)\n",
    "    return cont_log_probs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "785134f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cont_log_probs_sum': -40.21692657470703,\n",
       "  'cont_log_probs_mean': -5.745275020599365,\n",
       "  'n_cont_tokens': 7},\n",
       " {'cont_log_probs_sum': -38.0067253112793,\n",
       "  'cont_log_probs_mean': -6.33445405960083,\n",
       "  'n_cont_tokens': 6},\n",
       " {'cont_log_probs_sum': -3.460556745529175,\n",
       "  'cont_log_probs_mean': -3.460556745529175,\n",
       "  'n_cont_tokens': 1}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_log_probs(\n",
    "    prompts=[\"a shark ate every pirate\", \"a shark ate every pirate\",\"a shark ate every pirate\"],\n",
    "    continuations=[\"there is exactly one shark\", \"there are many sharks\",\".\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
