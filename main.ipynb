{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61b6f226",
   "metadata": {},
   "source": [
    "# Multilingual Semantics Probe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ea80eb",
   "metadata": {},
   "source": [
    "## Step 1: Corpus Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9961ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1c6656ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "STIMULI_DIR = \"./stimuli\"\n",
    "\n",
    "if not os.path.exists(STIMULI_DIR):\n",
    "    os.mkdir(STIMULI_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e5aba402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- English lexicon ---\n",
    "EN_SUBJECTS = [\n",
    "    \"shark\",\n",
    "    \"robot\",\n",
    "    \"chef\",\n",
    "    \"dog\",\n",
    "]\n",
    "\n",
    "EN_OBJECTS = [\n",
    "    \"pirate\",\n",
    "    \"student\",\n",
    "    \"doctor\",\n",
    "    \"tourist\",\n",
    "]\n",
    "\n",
    "# Use correct simple past forms\n",
    "EN_VERBS_PAST = [\n",
    "    \"ate\",\n",
    "    \"helped\",\n",
    "    \"pushed\",\n",
    "    \"chased\",\n",
    "]\n",
    "\n",
    "# --- Mandarin lexicon ---\n",
    "# Bare nouns only (no quantifiers inside)\n",
    "ZH_SUBJECTS = [\n",
    "    \"鲨鱼\",\n",
    "    \"机器人\",\n",
    "    \"厨师\",\n",
    "    \"狗\",\n",
    "]\n",
    "\n",
    "ZH_OBJECTS = [\n",
    "    \"海盗\",\n",
    "    \"学生\",\n",
    "    \"医生\",\n",
    "    \"游客\",\n",
    "]\n",
    "\n",
    "# Verb stems compatible with 了\n",
    "ZH_VERBS = [\n",
    "    \"吃\",\n",
    "    \"帮助\",\n",
    "    \"推\",\n",
    "    \"追\",\n",
    "]\n",
    "\n",
    "# Optional classifier map (defaults to 个)\n",
    "ZH_CLASSIFIER: Dict[str, str] = {\n",
    "    \"鲨鱼\": \"只\",\n",
    "    \"狗\": \"只\",\n",
    "    \"机器人\": \"个\",\n",
    "    \"厨师\": \"个\",\n",
    "    \"海盗\": \"个\",\n",
    "    \"学生\": \"个\",\n",
    "    \"医生\": \"个\",\n",
    "    \"游客\": \"个\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "44a60565",
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_TEMPLATES = [\n",
    "    # Classic ambiguous English form\n",
    "    \"A {subj} {verb_past} every {obj}.\",\n",
    "]\n",
    "\n",
    "ZH_TEMPLATES = [\n",
    "    # Canonical Mandarin surface-scope reading\n",
    "    \"有一{cl}{subj}{verb}了每个{obj}。\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2202da3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Stimulus:\n",
    "    language: str\n",
    "    template_id: str\n",
    "    subj: str\n",
    "    obj: str\n",
    "    verb: str\n",
    "    sentence: str\n",
    "\n",
    "    # Unique cross linguistic identifiers\n",
    "    tid: int\n",
    "    subj_i: int\n",
    "    obj_i: int\n",
    "    verb_i: int\n",
    "\n",
    "\n",
    "\n",
    "def get_classifier(noun: str, cl_map: Dict[str, str]) -> str:\n",
    "    return cl_map.get(noun, \"个\")\n",
    "\n",
    "\n",
    "def generate_english(\n",
    "    subjects: List[str],\n",
    "    objects: List[str],\n",
    "    verbs_past: List[str],\n",
    ") -> List[Stimulus]:\n",
    "    out: List[Stimulus] = []\n",
    "    for tid, tmpl in enumerate(EN_TEMPLATES):\n",
    "        for subj_i, subj in enumerate(subjects):\n",
    "            for obj_i, obj in enumerate(objects):\n",
    "                for verb_i, verb in enumerate(verbs_past):\n",
    "                    out.append(\n",
    "                        Stimulus(\n",
    "                            language=\"en\",\n",
    "                            template_id=f\"en_{tid}\",\n",
    "                            subj=subj,\n",
    "                            obj=obj,\n",
    "                            verb=verb,\n",
    "                            sentence=tmpl.format(\n",
    "                                subj=subj,\n",
    "                                obj=obj,\n",
    "                                verb_past=verb,\n",
    "                            ),\n",
    "                            tid=tid,\n",
    "                            subj_i=subj_i,\n",
    "                            obj_i=obj_i,\n",
    "                            verb_i=verb_i,\n",
    "                        )\n",
    "                    )\n",
    "    return out\n",
    "\n",
    "def generate_mandarin(\n",
    "    subjects: List[str],\n",
    "    objects: List[str],\n",
    "    verbs: List[str],\n",
    "    cl_map: Dict[str, str],\n",
    ") -> List[Stimulus]:\n",
    "    out: List[Stimulus] = []\n",
    "    for tid, tmpl in enumerate(ZH_TEMPLATES):\n",
    "        for subj_i, subj in enumerate(subjects):\n",
    "            for obj_i, obj in enumerate(objects):\n",
    "                for verb_i, verb in enumerate(verbs):\n",
    "                    cl = get_classifier(subj, cl_map)\n",
    "                    out.append(\n",
    "                        Stimulus(\n",
    "                            language=\"zh\",\n",
    "                            template_id=f\"zh_{tid}\",\n",
    "                            subj=subj,\n",
    "                            obj=obj,\n",
    "                            verb=verb,\n",
    "                            sentence=tmpl.format(\n",
    "                                cl=cl,\n",
    "                                subj=subj,\n",
    "                                obj=obj,\n",
    "                                verb=verb,\n",
    "                            ),\n",
    "                            tid=tid,\n",
    "                            subj_i=subj_i,\n",
    "                            obj_i=obj_i,\n",
    "                            verb_i=verb_i,\n",
    "                        )\n",
    "                    )\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d6e34c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stimuli = []\n",
    "stimuli += generate_english(EN_SUBJECTS, EN_OBJECTS, EN_VERBS_PAST)\n",
    "stimuli += generate_mandarin(ZH_SUBJECTS, ZH_OBJECTS, ZH_VERBS, ZH_CLASSIFIER)\n",
    "\n",
    "continuation_df = pd.DataFrame([s.__dict__ for s in stimuli])\n",
    "\n",
    "# Language-invariant semantic ID\n",
    "continuation_df[\"pair_id\"] = continuation_df.apply(\n",
    "    lambda r: f\"t{r.tid}_s{r.subj_i}_o{r.obj_i}_v{r.verb_i}\",\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Final unique ID\n",
    "continuation_df.insert(\n",
    "    0,\n",
    "    \"stimulus_id\",\n",
    "    continuation_df.apply(\n",
    "        lambda r: f\"{r.language}-{r.pair_id}\",\n",
    "        axis=1,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "186f7040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total stimuli: 128\n",
      "language\n",
      "en    64\n",
      "zh    64\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stimulus_id</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>en-t0_s2_o3_v1</td>\n",
       "      <td>A chef helped every tourist.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>en-t0_s1_o3_v1</td>\n",
       "      <td>A robot helped every tourist.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>en-t0_s2_o2_v3</td>\n",
       "      <td>A chef chased every doctor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>en-t0_s3_o3_v1</td>\n",
       "      <td>A dog helped every tourist.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>en-t0_s2_o0_v2</td>\n",
       "      <td>A chef pushed every pirate.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       stimulus_id                       sentence\n",
       "45  en-t0_s2_o3_v1   A chef helped every tourist.\n",
       "29  en-t0_s1_o3_v1  A robot helped every tourist.\n",
       "43  en-t0_s2_o2_v3    A chef chased every doctor.\n",
       "61  en-t0_s3_o3_v1    A dog helped every tourist.\n",
       "34  en-t0_s2_o0_v2    A chef pushed every pirate."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stimulus_id</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>zh-t0_s2_o3_v1</td>\n",
       "      <td>有一个厨师帮助了每个游客。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>zh-t0_s1_o3_v1</td>\n",
       "      <td>有一个机器人帮助了每个游客。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>zh-t0_s2_o2_v3</td>\n",
       "      <td>有一个厨师追了每个医生。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>zh-t0_s3_o3_v1</td>\n",
       "      <td>有一只狗帮助了每个游客。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>zh-t0_s2_o0_v2</td>\n",
       "      <td>有一个厨师推了每个海盗。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        stimulus_id        sentence\n",
       "109  zh-t0_s2_o3_v1   有一个厨师帮助了每个游客。\n",
       "93   zh-t0_s1_o3_v1  有一个机器人帮助了每个游客。\n",
       "107  zh-t0_s2_o2_v3    有一个厨师追了每个医生。\n",
       "125  zh-t0_s3_o3_v1    有一只狗帮助了每个游客。\n",
       "98   zh-t0_s2_o0_v2    有一个厨师推了每个海盗。"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Total stimuli:\", len(continuation_df))\n",
    "print(continuation_df[\"language\"].value_counts())\n",
    "\n",
    "display(\n",
    "    continuation_df[continuation_df[\"language\"] == \"en\"][[\"stimulus_id\", \"sentence\"]].sample(\n",
    "        min(5, (continuation_df[\"language\"] == \"en\").sum()),\n",
    "        random_state=0,\n",
    "    )\n",
    ")\n",
    "\n",
    "display(\n",
    "    continuation_df[continuation_df[\"language\"] == \"zh\"][[\"stimulus_id\", \"sentence\"]].sample(\n",
    "        min(5, (continuation_df[\"language\"] == \"zh\").sum()),\n",
    "        random_state=0,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "891de94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote stimuli.csv and stimuli.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Serialize\n",
    "continuation_df.to_csv(os.path.join(STIMULI_DIR,\"stimuli.csv\"), index=False)\n",
    "\n",
    "with open(os.path.join(STIMULI_DIR, \"stimuli.jsonl\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    for row in continuation_df.to_dict(orient=\"records\"):\n",
    "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Wrote stimuli.csv and stimuli.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79665af",
   "metadata": {},
   "source": [
    "### Add Natural Language Continuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b9bc3d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_CONTINUATIONS = {\n",
    "    \"surface\": \" There was only one {subj}.\",\n",
    "    \"inverse\": \" There were many {subj}.\",\n",
    "}\n",
    "\n",
    "# Mandarin: keep equally short.\n",
    "# Note: plural is usually implicit; \"很多\" is a decent lexical cue.\n",
    "ZH_CONTINUATIONS = {\n",
    "    \"surface\": \" 只有一{cl}{subj}。\",\n",
    "    \"inverse\": \" 有很多{cl}{subj}。\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "98b3708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_continuations(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for r in df.itertuples(index=False):\n",
    "        base = r._asdict()\n",
    "\n",
    "        if base[\"language\"] == \"en\":\n",
    "            # naive pluralization: add \"s\"\n",
    "            # If you care about irregular plurals later, add a map.\n",
    "            subj_plural = base[\"subj\"] + \"s\"\n",
    "            cont_map = {\n",
    "                \"surface\": EN_CONTINUATIONS[\"surface\"].format(subj=base[\"subj\"]),\n",
    "                \"inverse\": EN_CONTINUATIONS[\"inverse\"].format(subj=subj_plural),\n",
    "            }\n",
    "\n",
    "        elif base[\"language\"] == \"zh\":\n",
    "            cl = ZH_CLASSIFIER.get(base[\"subj\"], \"个\")\n",
    "            cont_map = {\n",
    "                \"surface\": ZH_CONTINUATIONS[\"surface\"].format(cl=cl, subj=base[\"subj\"]),\n",
    "                \"inverse\": ZH_CONTINUATIONS[\"inverse\"].format(cl=cl, subj=base[\"subj\"]),\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown language: {base['language']}\")\n",
    "\n",
    "        for cont_type, cont_text in cont_map.items():\n",
    "            ex = dict(base)\n",
    "            ex[\"continuation_type\"] = cont_type            # \"surface\" or \"inverse\"\n",
    "            ex[\"continuation_text\"] = cont_text            # the thing you'll score\n",
    "            ex[\"full_text\"] = base[\"sentence\"] + cont_text # convenient for debugging\n",
    "            rows.append(ex)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "217600e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cont = add_continuations(continuation_df)\n",
    "\n",
    "if \"concept_id\" not in df_cont.columns:\n",
    "    concept_series = df_cont[\"subj\"] + \"|\" + df_cont[\"obj\"] + \"|\" + df_cont[\"verb\"]\n",
    "    df_cont.insert(1, \"concept_id\", concept_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d6994fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote stimuli_with_continuations.csv and stimuli_with_continuations.jsonl\n"
     ]
    }
   ],
   "source": [
    "df_cont.to_csv(os.path.join(STIMULI_DIR, \"stimuli_with_continuations.csv\"), index=False)\n",
    "\n",
    "with open(os.path.join(STIMULI_DIR, \"stimuli_with_continuations.jsonl\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    for row in df_cont.to_dict(orient=\"records\"):\n",
    "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Wrote stimuli_with_continuations.csv and stimuli_with_continuations.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7115d99",
   "metadata": {},
   "source": [
    "## Step 2: Evaluate Log Probs of Continuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eb7f0a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e639abf",
   "metadata": {},
   "source": [
    "### Load in Continuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f139f006",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSONL_PATH = \"stimuli/stimuli_with_continuations.jsonl\"\n",
    "\n",
    "def read_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                yield json.loads(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f29998aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = list(read_jsonl(JSONL_PATH))\n",
    "continuation_df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c05e25",
   "metadata": {},
   "source": [
    "### Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a52ea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt2\"\n",
    "LOWER_CASE = True\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.bfloat16 if (DEVICE == \"cuda\") else torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1889b2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0d341895",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_CSV = \"logprob_surface_vs_inverse.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4b7e0983",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a26a9251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=DTYPE\n",
    ").to(DEVICE)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae5f1d3",
   "metadata": {},
   "source": [
    "### Calculate Continuation Log Prob\n",
    "1) Tokenize Prompt and Continuation\n",
    "\n",
    "2) Calculate Log Probs for prompt + continuation\n",
    "\n",
    "3) Calculate continuation log probs: [a] get log prob of each next token from [0,T-1]; [b] sum log probs\n",
    "- Given a prompt of length T, log_prob[:, :T-1, :] gives the probability of the first [0,T-1] tokens\n",
    "\n",
    "```\n",
    "\n",
    "a shark ate every pirate\n",
    "                    ^ stop here; we already know the probability of this word\n",
    "```\n",
    "\n",
    "```python\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        target_log_probs[b,t] = shifted_log_probs[b, t, target_tokens[b,t]]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba1b175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_probs(prompts: list[str], continuations: list[str]) -> list[dict]:\n",
    "    # 1) Tokenize Prompt and Continuation\n",
    "    enc_base_prompts = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "\n",
    "        # Handle different length prompts\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "\n",
    "        # Avoid [EOS]/[BOS] from being inserted\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "\n",
    "    full_prompts = [p + c for p, c in zip(prompts, continuations)]\n",
    "    \n",
    "    full_prompts = [p.lower() for p in full_prompts] if LOWER_CASE else full_prompts\n",
    "    \n",
    "    enc_full_prompts = tokenizer(\n",
    "        full_prompts,\n",
    "        return_tensors=\"pt\",\n",
    "\n",
    "        # Handle different length prompts\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "\n",
    "        # Avoid [EOS]/[BOS] from being inserted\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "\n",
    "    input_ids = enc_full_prompts[\"input_ids\"].to(DEVICE)\n",
    "    attention_mask = enc_full_prompts[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "    # 2) Calculate logProbs for prompt + continuation\n",
    "    out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = out.logits.to(torch.float32)  # [B, T, V]\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)  # [B, T, V]\n",
    "\n",
    "    # 3) Calculate continuation log probs: [a] get log prob of each next token from [0,T-1]; [b] sum log probs\n",
    "    target_tokens = input_ids[:, 1:]  # [B, T-1]\n",
    "    shifted_log_probs = log_probs[:, :-1, :]  # [B, T-1, V]\n",
    "\n",
    "    # Select the logProb for the selected token\n",
    "    target_log_probs = torch.gather(\n",
    "        input=shifted_log_probs,\n",
    "        dim=-1,  # Select the log_prob for the selected prompt token in vocab\n",
    "        index=target_tokens.unsqueeze(-1)  # [B, T-1, 1]\n",
    "    ).squeeze(-1)  # [B, T-1]\n",
    "\n",
    "    base_prompt_lens = enc_base_prompts[\"attention_mask\"].sum(dim=1).tolist()\n",
    "    full_prompt_lens = enc_full_prompts[\"attention_mask\"].sum(dim=1).tolist()\n",
    "\n",
    "    # The logProbs for the continuation live at (inclusive)\n",
    "    # logits[m-1:n-2] -> logits[m-1:L-2] -> target_log_probs[m-1:L-1] since we already removed one token in the shift\n",
    "    B, _ = target_log_probs.shape\n",
    "\n",
    "    cont_log_probs_list = []\n",
    "\n",
    "    for b in range(B):\n",
    "        base_prompt_length = base_prompt_lens[b]\n",
    "        full_prompt_length = full_prompt_lens[b]\n",
    "\n",
    "        cont_log_probs = target_log_probs[b,\n",
    "                                          base_prompt_length-1:full_prompt_length-1]\n",
    "        cont_log_probs_sum = cont_log_probs.sum().item()\n",
    "        cont_log_probs_mean = cont_log_probs.mean().item()\n",
    "        n_cont_tokens = full_prompt_length - base_prompt_length  # Sanity Check\n",
    "\n",
    "        cont_log_probs_list.append(\n",
    "            {\"cont_log_probs_sum\": cont_log_probs_sum,\n",
    "                \"cont_log_probs_mean\": cont_log_probs_mean,\n",
    "                \"n_cont_tokens\": n_cont_tokens\n",
    "             }\n",
    "        )\n",
    "    return cont_log_probs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5a7c57b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_log_probs(df: pd.DataFrame, b_sz: int = BATCH_SIZE):\n",
    "    df = df.reset_index(drop=True).copy() # Pandas safety reasons\n",
    "    all_stats = []\n",
    "\n",
    "    for start in range(0, len(df), b_sz):\n",
    "        end = min(start + b_sz, len(df))\n",
    "        batch = continuation_df.iloc[start:end]\n",
    "\n",
    "        stats = get_log_probs(\n",
    "            prompts=batch[\"sentence\"].tolist(),\n",
    "            continuations=batch[\"continuation_text\"].tolist()\n",
    "        )\n",
    "\n",
    "        all_stats += stats\n",
    "\n",
    "    stats_df = pd.DataFrame(all_stats)    \n",
    "\n",
    "    return pd.concat([df, stats_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dae38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI Generated pivot table magic to get the metrics\n",
    "def collapse_surface_inverse(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df[df[\"continuation_type\"].isin([\"surface\", \"inverse\"])].copy()\n",
    "\n",
    "    num_cols  = [\"cont_log_probs_sum\", \"cont_log_probs_mean\", \"n_cont_tokens\"]\n",
    "    text_cols = [\"continuation_text\", \"full_text\"]\n",
    "\n",
    "    # metadata that is shared across types\n",
    "    meta_cols = [\n",
    "        c for c in df.columns\n",
    "        if c not in ([\"continuation_type\"] + num_cols + text_cols)\n",
    "    ]\n",
    "    meta = df.groupby(\"stimulus_id\", as_index=False)[meta_cols].first()\n",
    "\n",
    "    # --- 1) pivot numeric columns (keeps float/int dtypes) ---\n",
    "    wide_num = (\n",
    "        df[[\"stimulus_id\", \"continuation_type\"] + num_cols]\n",
    "        .pivot(index=\"stimulus_id\", columns=\"continuation_type\", values=num_cols)\n",
    "    )\n",
    "    wide_num.columns = [f\"{col}_{ctype}\" for col, ctype in wide_num.columns]\n",
    "    wide_num = wide_num.reset_index()\n",
    "\n",
    "    # --- 2) pivot text columns (object dtype is fine here) ---\n",
    "    wide_text = (\n",
    "        df[[\"stimulus_id\", \"continuation_type\"] + text_cols]\n",
    "        .pivot(index=\"stimulus_id\", columns=\"continuation_type\", values=text_cols)\n",
    "    )\n",
    "    wide_text.columns = [f\"{col}_{ctype}\" for col, ctype in wide_text.columns]\n",
    "    wide_text = wide_text.reset_index()\n",
    "\n",
    "    # merge everything\n",
    "    out = meta.merge(wide_num, on=\"stimulus_id\", how=\"left\") \\\n",
    "              .merge(wide_text, on=\"stimulus_id\", how=\"left\")\n",
    "\n",
    "    # deltas + ratios (now safe: numeric dtypes)\n",
    "    out[\"delta_sum\"]  = out[\"cont_log_probs_sum_inverse\"]  - out[\"cont_log_probs_sum_surface\"]\n",
    "    out[\"ratio_sum\"]  = np.exp(out[\"delta_sum\"])\n",
    "\n",
    "    out[\"delta_mean\"] = out[\"cont_log_probs_mean_inverse\"] - out[\"cont_log_probs_mean_surface\"]\n",
    "    out[\"ratio_mean\"] = np.exp(out[\"delta_mean\"])\n",
    "\n",
    "    out[\"preference_sum\"] = np.select(\n",
    "        [out[\"delta_sum\"] > 0, out[\"delta_sum\"] < 0],\n",
    "        [\"inverse\", \"surface\"],\n",
    "        default=\"tie\"\n",
    "    )\n",
    "\n",
    "    out[\"preference_mean\"] = np.select(\n",
    "        [out[\"delta_mean\"] > 0, out[\"delta_mean\"] < 0],\n",
    "        [\"inverse\", \"surface\"],\n",
    "        default=\"tie\"\n",
    "    )\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d694d592",
   "metadata": {},
   "source": [
    "### Save Outputs\n",
    "- Add model name as a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5cda37e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "RESULTS_DIR = Path(\"./results\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b7e51e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "safe_model = re.sub(r\"[^\\w\\-\\.]\", \"_\", MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "29d6b81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_long = process_log_probs(continuation_df, b_sz=BATCH_SIZE)\n",
    "scored_long.insert(0,\"model\", safe_model)\n",
    "\n",
    "scored_wide = collapse_surface_inverse(scored_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b64c6e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>stimulus_id</th>\n",
       "      <th>concept_id</th>\n",
       "      <th>language</th>\n",
       "      <th>template_id</th>\n",
       "      <th>subj</th>\n",
       "      <th>obj</th>\n",
       "      <th>verb</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tid</th>\n",
       "      <th>...</th>\n",
       "      <th>continuation_text_inverse</th>\n",
       "      <th>continuation_text_surface</th>\n",
       "      <th>full_text_inverse</th>\n",
       "      <th>full_text_surface</th>\n",
       "      <th>delta_sum</th>\n",
       "      <th>ratio_sum</th>\n",
       "      <th>delta_mean</th>\n",
       "      <th>ratio_mean</th>\n",
       "      <th>preference_sum</th>\n",
       "      <th>preference_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>en-t0_s0_o0_v0</td>\n",
       "      <td>shark|pirate|ate</td>\n",
       "      <td>en</td>\n",
       "      <td>en_0</td>\n",
       "      <td>shark</td>\n",
       "      <td>pirate</td>\n",
       "      <td>ate</td>\n",
       "      <td>A shark ate every pirate.</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>There were many sharks.</td>\n",
       "      <td>There was only one shark.</td>\n",
       "      <td>A shark ate every pirate. There were many sharks.</td>\n",
       "      <td>A shark ate every pirate. There was only one s...</td>\n",
       "      <td>-1.176224</td>\n",
       "      <td>0.308441</td>\n",
       "      <td>-0.751752</td>\n",
       "      <td>0.471540</td>\n",
       "      <td>surface</td>\n",
       "      <td>surface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>en-t0_s0_o0_v1</td>\n",
       "      <td>shark|pirate|helped</td>\n",
       "      <td>en</td>\n",
       "      <td>en_0</td>\n",
       "      <td>shark</td>\n",
       "      <td>pirate</td>\n",
       "      <td>helped</td>\n",
       "      <td>A shark helped every pirate.</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>There were many sharks.</td>\n",
       "      <td>There was only one shark.</td>\n",
       "      <td>A shark helped every pirate. There were many s...</td>\n",
       "      <td>A shark helped every pirate. There was only on...</td>\n",
       "      <td>-0.826022</td>\n",
       "      <td>0.437787</td>\n",
       "      <td>-0.678922</td>\n",
       "      <td>0.507163</td>\n",
       "      <td>surface</td>\n",
       "      <td>surface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>en-t0_s0_o0_v2</td>\n",
       "      <td>shark|pirate|pushed</td>\n",
       "      <td>en</td>\n",
       "      <td>en_0</td>\n",
       "      <td>shark</td>\n",
       "      <td>pirate</td>\n",
       "      <td>pushed</td>\n",
       "      <td>A shark pushed every pirate.</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>There were many sharks.</td>\n",
       "      <td>There was only one shark.</td>\n",
       "      <td>A shark pushed every pirate. There were many s...</td>\n",
       "      <td>A shark pushed every pirate. There was only on...</td>\n",
       "      <td>-0.633678</td>\n",
       "      <td>0.530636</td>\n",
       "      <td>-0.639003</td>\n",
       "      <td>0.527818</td>\n",
       "      <td>surface</td>\n",
       "      <td>surface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>en-t0_s0_o0_v3</td>\n",
       "      <td>shark|pirate|chased</td>\n",
       "      <td>en</td>\n",
       "      <td>en_0</td>\n",
       "      <td>shark</td>\n",
       "      <td>pirate</td>\n",
       "      <td>chased</td>\n",
       "      <td>A shark chased every pirate.</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>There were many sharks.</td>\n",
       "      <td>There was only one shark.</td>\n",
       "      <td>A shark chased every pirate. There were many s...</td>\n",
       "      <td>A shark chased every pirate. There was only on...</td>\n",
       "      <td>-0.457541</td>\n",
       "      <td>0.632838</td>\n",
       "      <td>-0.585751</td>\n",
       "      <td>0.556688</td>\n",
       "      <td>surface</td>\n",
       "      <td>surface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>en-t0_s0_o1_v0</td>\n",
       "      <td>shark|student|ate</td>\n",
       "      <td>en</td>\n",
       "      <td>en_0</td>\n",
       "      <td>shark</td>\n",
       "      <td>student</td>\n",
       "      <td>ate</td>\n",
       "      <td>A shark ate every student.</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>There were many sharks.</td>\n",
       "      <td>There was only one shark.</td>\n",
       "      <td>A shark ate every student. There were many sha...</td>\n",
       "      <td>A shark ate every student. There was only one ...</td>\n",
       "      <td>-1.976831</td>\n",
       "      <td>0.138507</td>\n",
       "      <td>-0.888041</td>\n",
       "      <td>0.411461</td>\n",
       "      <td>surface</td>\n",
       "      <td>surface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>zh-t0_s3_o2_v3</td>\n",
       "      <td>狗|医生|追</td>\n",
       "      <td>zh</td>\n",
       "      <td>zh_0</td>\n",
       "      <td>狗</td>\n",
       "      <td>医生</td>\n",
       "      <td>追</td>\n",
       "      <td>有一只狗追了每个医生。</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>有很多只狗。</td>\n",
       "      <td>只有一只狗。</td>\n",
       "      <td>有一只狗追了每个医生。 有很多只狗。</td>\n",
       "      <td>有一只狗追了每个医生。 只有一只狗。</td>\n",
       "      <td>-12.770380</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-1.160944</td>\n",
       "      <td>0.313191</td>\n",
       "      <td>surface</td>\n",
       "      <td>surface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>zh-t0_s3_o3_v0</td>\n",
       "      <td>狗|游客|吃</td>\n",
       "      <td>zh</td>\n",
       "      <td>zh_0</td>\n",
       "      <td>狗</td>\n",
       "      <td>游客</td>\n",
       "      <td>吃</td>\n",
       "      <td>有一只狗吃了每个游客。</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>有很多只狗。</td>\n",
       "      <td>只有一只狗。</td>\n",
       "      <td>有一只狗吃了每个游客。 有很多只狗。</td>\n",
       "      <td>有一只狗吃了每个游客。 只有一只狗。</td>\n",
       "      <td>-13.137228</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-1.194293</td>\n",
       "      <td>0.302918</td>\n",
       "      <td>surface</td>\n",
       "      <td>surface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>zh-t0_s3_o3_v1</td>\n",
       "      <td>狗|游客|帮助</td>\n",
       "      <td>zh</td>\n",
       "      <td>zh_0</td>\n",
       "      <td>狗</td>\n",
       "      <td>游客</td>\n",
       "      <td>帮助</td>\n",
       "      <td>有一只狗帮助了每个游客。</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>有很多只狗。</td>\n",
       "      <td>只有一只狗。</td>\n",
       "      <td>有一只狗帮助了每个游客。 有很多只狗。</td>\n",
       "      <td>有一只狗帮助了每个游客。 只有一只狗。</td>\n",
       "      <td>-11.615507</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>-1.055955</td>\n",
       "      <td>0.347860</td>\n",
       "      <td>surface</td>\n",
       "      <td>surface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>zh-t0_s3_o3_v2</td>\n",
       "      <td>狗|游客|推</td>\n",
       "      <td>zh</td>\n",
       "      <td>zh_0</td>\n",
       "      <td>狗</td>\n",
       "      <td>游客</td>\n",
       "      <td>推</td>\n",
       "      <td>有一只狗推了每个游客。</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>有很多只狗。</td>\n",
       "      <td>只有一只狗。</td>\n",
       "      <td>有一只狗推了每个游客。 有很多只狗。</td>\n",
       "      <td>有一只狗推了每个游客。 只有一只狗。</td>\n",
       "      <td>-12.681486</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-1.152862</td>\n",
       "      <td>0.315732</td>\n",
       "      <td>surface</td>\n",
       "      <td>surface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>zh-t0_s3_o3_v3</td>\n",
       "      <td>狗|游客|追</td>\n",
       "      <td>zh</td>\n",
       "      <td>zh_0</td>\n",
       "      <td>狗</td>\n",
       "      <td>游客</td>\n",
       "      <td>追</td>\n",
       "      <td>有一只狗追了每个游客。</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>有很多只狗。</td>\n",
       "      <td>只有一只狗。</td>\n",
       "      <td>有一只狗追了每个游客。 有很多只狗。</td>\n",
       "      <td>有一只狗追了每个游客。 只有一只狗。</td>\n",
       "      <td>-12.937635</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-1.176149</td>\n",
       "      <td>0.308464</td>\n",
       "      <td>surface</td>\n",
       "      <td>surface</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    model     stimulus_id           concept_id language template_id   subj  \\\n",
       "0    gpt2  en-t0_s0_o0_v0     shark|pirate|ate       en        en_0  shark   \n",
       "1    gpt2  en-t0_s0_o0_v1  shark|pirate|helped       en        en_0  shark   \n",
       "2    gpt2  en-t0_s0_o0_v2  shark|pirate|pushed       en        en_0  shark   \n",
       "3    gpt2  en-t0_s0_o0_v3  shark|pirate|chased       en        en_0  shark   \n",
       "4    gpt2  en-t0_s0_o1_v0    shark|student|ate       en        en_0  shark   \n",
       "..    ...             ...                  ...      ...         ...    ...   \n",
       "123  gpt2  zh-t0_s3_o2_v3               狗|医生|追       zh        zh_0      狗   \n",
       "124  gpt2  zh-t0_s3_o3_v0               狗|游客|吃       zh        zh_0      狗   \n",
       "125  gpt2  zh-t0_s3_o3_v1              狗|游客|帮助       zh        zh_0      狗   \n",
       "126  gpt2  zh-t0_s3_o3_v2               狗|游客|推       zh        zh_0      狗   \n",
       "127  gpt2  zh-t0_s3_o3_v3               狗|游客|追       zh        zh_0      狗   \n",
       "\n",
       "         obj    verb                      sentence  tid  ...  \\\n",
       "0     pirate     ate     A shark ate every pirate.    0  ...   \n",
       "1     pirate  helped  A shark helped every pirate.    0  ...   \n",
       "2     pirate  pushed  A shark pushed every pirate.    0  ...   \n",
       "3     pirate  chased  A shark chased every pirate.    0  ...   \n",
       "4    student     ate    A shark ate every student.    0  ...   \n",
       "..       ...     ...                           ...  ...  ...   \n",
       "123       医生       追                   有一只狗追了每个医生。    0  ...   \n",
       "124       游客       吃                   有一只狗吃了每个游客。    0  ...   \n",
       "125       游客      帮助                  有一只狗帮助了每个游客。    0  ...   \n",
       "126       游客       推                   有一只狗推了每个游客。    0  ...   \n",
       "127       游客       追                   有一只狗追了每个游客。    0  ...   \n",
       "\n",
       "     continuation_text_inverse   continuation_text_surface  \\\n",
       "0      There were many sharks.   There was only one shark.   \n",
       "1      There were many sharks.   There was only one shark.   \n",
       "2      There were many sharks.   There was only one shark.   \n",
       "3      There were many sharks.   There was only one shark.   \n",
       "4      There were many sharks.   There was only one shark.   \n",
       "..                         ...                         ...   \n",
       "123                     有很多只狗。                      只有一只狗。   \n",
       "124                     有很多只狗。                      只有一只狗。   \n",
       "125                     有很多只狗。                      只有一只狗。   \n",
       "126                     有很多只狗。                      只有一只狗。   \n",
       "127                     有很多只狗。                      只有一只狗。   \n",
       "\n",
       "                                     full_text_inverse  \\\n",
       "0    A shark ate every pirate. There were many sharks.   \n",
       "1    A shark helped every pirate. There were many s...   \n",
       "2    A shark pushed every pirate. There were many s...   \n",
       "3    A shark chased every pirate. There were many s...   \n",
       "4    A shark ate every student. There were many sha...   \n",
       "..                                                 ...   \n",
       "123                                 有一只狗追了每个医生。 有很多只狗。   \n",
       "124                                 有一只狗吃了每个游客。 有很多只狗。   \n",
       "125                                有一只狗帮助了每个游客。 有很多只狗。   \n",
       "126                                 有一只狗推了每个游客。 有很多只狗。   \n",
       "127                                 有一只狗追了每个游客。 有很多只狗。   \n",
       "\n",
       "                                     full_text_surface  delta_sum  ratio_sum  \\\n",
       "0    A shark ate every pirate. There was only one s...  -1.176224   0.308441   \n",
       "1    A shark helped every pirate. There was only on...  -0.826022   0.437787   \n",
       "2    A shark pushed every pirate. There was only on...  -0.633678   0.530636   \n",
       "3    A shark chased every pirate. There was only on...  -0.457541   0.632838   \n",
       "4    A shark ate every student. There was only one ...  -1.976831   0.138507   \n",
       "..                                                 ...        ...        ...   \n",
       "123                                 有一只狗追了每个医生。 只有一只狗。 -12.770380   0.000003   \n",
       "124                                 有一只狗吃了每个游客。 只有一只狗。 -13.137228   0.000002   \n",
       "125                                有一只狗帮助了每个游客。 只有一只狗。 -11.615507   0.000009   \n",
       "126                                 有一只狗推了每个游客。 只有一只狗。 -12.681486   0.000003   \n",
       "127                                 有一只狗追了每个游客。 只有一只狗。 -12.937635   0.000002   \n",
       "\n",
       "     delta_mean  ratio_mean  preference_sum  preference_mean  \n",
       "0     -0.751752    0.471540         surface          surface  \n",
       "1     -0.678922    0.507163         surface          surface  \n",
       "2     -0.639003    0.527818         surface          surface  \n",
       "3     -0.585751    0.556688         surface          surface  \n",
       "4     -0.888041    0.411461         surface          surface  \n",
       "..          ...         ...             ...              ...  \n",
       "123   -1.160944    0.313191         surface          surface  \n",
       "124   -1.194293    0.302918         surface          surface  \n",
       "125   -1.055955    0.347860         surface          surface  \n",
       "126   -1.152862    0.315732         surface          surface  \n",
       "127   -1.176149    0.308464         surface          surface  \n",
       "\n",
       "[128 rows x 30 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scored_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "59f4e4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_long.to_json(\n",
    "    RESULTS_DIR / f\"scored_long_{safe_model}.jsonl\",\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    "    force_ascii=False\n",
    ")\n",
    "\n",
    "scored_wide.to_json(\n",
    "    RESULTS_DIR / f\"scored_wide_{safe_model}.jsonl\",\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    "    force_ascii=False\n",
    ")\n",
    "\n",
    "# with open(os.path.join(STIMULI_DIR, RESULTS_DIR / f\"scored_wide_{safe_model}.jsonl\"), \"w\", encoding=\"utf-8\") as f:\n",
    "#     for row in scored_wide.to_dict(orient=\"records\"):\n",
    "#         f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# print(\"Wrote stimuli.csv and stimuli.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
