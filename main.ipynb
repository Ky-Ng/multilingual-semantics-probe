{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61b6f226",
   "metadata": {},
   "source": [
    "# Multilingual Semantics Probe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ea80eb",
   "metadata": {},
   "source": [
    "## Step 1: Corpus Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c9961ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import itertools\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1c6656ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "STIMULI_DIR = \"./stimuli\"\n",
    "\n",
    "if not os.path.exists(STIMULI_DIR):\n",
    "    os.mkdir(STIMULI_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e5aba402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- English lexicon ---\n",
    "EN_SUBJECTS = [\n",
    "    \"shark\",\n",
    "    \"robot\",\n",
    "    \"chef\",\n",
    "    \"dog\",\n",
    "]\n",
    "\n",
    "EN_OBJECTS = [\n",
    "    \"pirate\",\n",
    "    \"student\",\n",
    "    \"doctor\",\n",
    "    \"tourist\",\n",
    "]\n",
    "\n",
    "# Use correct simple past forms\n",
    "EN_VERBS_PAST = [\n",
    "    \"ate\",\n",
    "    \"helped\",\n",
    "    \"pushed\",\n",
    "    \"chased\",\n",
    "]\n",
    "\n",
    "# --- Mandarin lexicon ---\n",
    "# Bare nouns only (no quantifiers inside)\n",
    "ZH_SUBJECTS = [\n",
    "    \"鲨鱼\",\n",
    "    \"机器人\",\n",
    "    \"厨师\",\n",
    "    \"狗\",\n",
    "]\n",
    "\n",
    "ZH_OBJECTS = [\n",
    "    \"海盗\",\n",
    "    \"学生\",\n",
    "    \"医生\",\n",
    "    \"游客\",\n",
    "]\n",
    "\n",
    "# Verb stems compatible with 了\n",
    "ZH_VERBS = [\n",
    "    \"吃\",\n",
    "    \"帮助\",\n",
    "    \"推\",\n",
    "    \"追\",\n",
    "]\n",
    "\n",
    "# Optional classifier map (defaults to 个)\n",
    "ZH_CLASSIFIER: Dict[str, str] = {\n",
    "    \"鲨鱼\": \"只\",\n",
    "    \"狗\": \"只\",\n",
    "    \"机器人\": \"个\",\n",
    "    \"厨师\": \"个\",\n",
    "    \"海盗\": \"个\",\n",
    "    \"学生\": \"个\",\n",
    "    \"医生\": \"个\",\n",
    "    \"游客\": \"个\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "44a60565",
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_TEMPLATES = [\n",
    "    # Classic ambiguous English form\n",
    "    \"A {subj} {verb_past} every {obj}.\",\n",
    "]\n",
    "\n",
    "ZH_TEMPLATES = [\n",
    "    # Canonical Mandarin surface-scope reading\n",
    "    \"有一{cl}{subj}{verb}了每个{obj}。\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "2202da3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Stimulus:\n",
    "    language: str\n",
    "    template_id: str\n",
    "    subj: str\n",
    "    obj: str\n",
    "    verb: str\n",
    "    sentence: str\n",
    "\n",
    "\n",
    "def get_classifier(noun: str, cl_map: Dict[str, str]) -> str:\n",
    "    return cl_map.get(noun, \"个\")\n",
    "\n",
    "\n",
    "def generate_english(\n",
    "    subjects: List[str],\n",
    "    objects: List[str],\n",
    "    verbs_past: List[str],\n",
    ") -> List[Stimulus]:\n",
    "    out: List[Stimulus] = []\n",
    "    for tid, tmpl in enumerate(EN_TEMPLATES):\n",
    "        for subj, obj, verb in itertools.product(subjects, objects, verbs_past):\n",
    "            out.append(\n",
    "                Stimulus(\n",
    "                    language=\"en\",\n",
    "                    template_id=f\"en_{tid}\",\n",
    "                    subj=subj,\n",
    "                    obj=obj,\n",
    "                    verb=verb,\n",
    "                    sentence=tmpl.format(\n",
    "                        subj=subj,\n",
    "                        obj=obj,\n",
    "                        verb_past=verb,\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "    return out\n",
    "\n",
    "\n",
    "def generate_mandarin(\n",
    "    subjects: List[str],\n",
    "    objects: List[str],\n",
    "    verbs: List[str],\n",
    "    cl_map: Dict[str, str],\n",
    ") -> List[Stimulus]:\n",
    "    out: List[Stimulus] = []\n",
    "    for tid, tmpl in enumerate(ZH_TEMPLATES):\n",
    "        for subj, obj, verb in itertools.product(subjects, objects, verbs):\n",
    "            cl = get_classifier(subj, cl_map)\n",
    "            out.append(\n",
    "                Stimulus(\n",
    "                    language=\"zh\",\n",
    "                    template_id=f\"zh_{tid}\",\n",
    "                    subj=subj,\n",
    "                    obj=obj,\n",
    "                    verb=verb,\n",
    "                    sentence=tmpl.format(\n",
    "                        cl=cl,\n",
    "                        subj=subj,\n",
    "                        obj=obj,\n",
    "                        verb=verb,\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e34c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stimuli = []\n",
    "stimuli += generate_english(EN_SUBJECTS, EN_OBJECTS, EN_VERBS_PAST)\n",
    "stimuli += generate_mandarin(ZH_SUBJECTS, ZH_OBJECTS, ZH_VERBS, ZH_CLASSIFIER)\n",
    "\n",
    "continuation_df = pd.DataFrame([s.__dict__ for s in stimuli])\n",
    "\n",
    "# Stable IDs for downstream scoring\n",
    "continuation_df.insert(\n",
    "    0,\n",
    "    \"stimulus_id\",\n",
    "    [\n",
    "        f\"{row.language}-{row.template_id}-{row.Index:06d}\"\n",
    "        for row in continuation_df.itertuples()\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186f7040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total stimuli: 128\n",
      "language\n",
      "en    64\n",
      "zh    64\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stimulus_id</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>en-en_0-000045</td>\n",
       "      <td>A chef helped every tourist.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>en-en_0-000029</td>\n",
       "      <td>A robot helped every tourist.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>en-en_0-000043</td>\n",
       "      <td>A chef chased every doctor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>en-en_0-000061</td>\n",
       "      <td>A dog helped every tourist.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>en-en_0-000034</td>\n",
       "      <td>A chef pushed every pirate.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       stimulus_id                       sentence\n",
       "45  en-en_0-000045   A chef helped every tourist.\n",
       "29  en-en_0-000029  A robot helped every tourist.\n",
       "43  en-en_0-000043    A chef chased every doctor.\n",
       "61  en-en_0-000061    A dog helped every tourist.\n",
       "34  en-en_0-000034    A chef pushed every pirate."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stimulus_id</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>zh-zh_0-000109</td>\n",
       "      <td>有一个厨师帮助了每个游客。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>zh-zh_0-000093</td>\n",
       "      <td>有一个机器人帮助了每个游客。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>zh-zh_0-000107</td>\n",
       "      <td>有一个厨师追了每个医生。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>zh-zh_0-000125</td>\n",
       "      <td>有一只狗帮助了每个游客。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>zh-zh_0-000098</td>\n",
       "      <td>有一个厨师推了每个海盗。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        stimulus_id        sentence\n",
       "109  zh-zh_0-000109   有一个厨师帮助了每个游客。\n",
       "93   zh-zh_0-000093  有一个机器人帮助了每个游客。\n",
       "107  zh-zh_0-000107    有一个厨师追了每个医生。\n",
       "125  zh-zh_0-000125    有一只狗帮助了每个游客。\n",
       "98   zh-zh_0-000098    有一个厨师推了每个海盗。"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Total stimuli:\", len(continuation_df))\n",
    "print(continuation_df[\"language\"].value_counts())\n",
    "\n",
    "display(\n",
    "    continuation_df[continuation_df[\"language\"] == \"en\"][[\"stimulus_id\", \"sentence\"]].sample(\n",
    "        min(5, (continuation_df[\"language\"] == \"en\").sum()),\n",
    "        random_state=0,\n",
    "    )\n",
    ")\n",
    "\n",
    "display(\n",
    "    continuation_df[continuation_df[\"language\"] == \"zh\"][[\"stimulus_id\", \"sentence\"]].sample(\n",
    "        min(5, (continuation_df[\"language\"] == \"zh\").sum()),\n",
    "        random_state=0,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891de94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote stimuli.csv and stimuli.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Serialize\n",
    "continuation_df.to_csv(os.path.join(STIMULI_DIR,\"stimuli.csv\"), index=False)\n",
    "\n",
    "with open(os.path.join(STIMULI_DIR, \"stimuli.jsonl\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    for row in continuation_df.to_dict(orient=\"records\"):\n",
    "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Wrote stimuli.csv and stimuli.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79665af",
   "metadata": {},
   "source": [
    "### Add Natural Language Continuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b9bc3d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_CONTINUATIONS = {\n",
    "    \"surface\": \" There was only one {subj}.\",\n",
    "    \"inverse\": \" There were many {subj}.\",\n",
    "}\n",
    "\n",
    "# Mandarin: keep equally short.\n",
    "# Note: plural is usually implicit; \"很多\" is a decent lexical cue.\n",
    "ZH_CONTINUATIONS = {\n",
    "    \"surface\": \" 只有一{cl}{subj}。\",\n",
    "    \"inverse\": \" 有很多{cl}{subj}。\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "98b3708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_continuations(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for r in df.itertuples(index=False):\n",
    "        base = r._asdict()\n",
    "\n",
    "        if base[\"language\"] == \"en\":\n",
    "            # naive pluralization: add \"s\"\n",
    "            # If you care about irregular plurals later, add a map.\n",
    "            subj_plural = base[\"subj\"] + \"s\"\n",
    "            cont_map = {\n",
    "                \"surface\": EN_CONTINUATIONS[\"surface\"].format(subj=base[\"subj\"]),\n",
    "                \"inverse\": EN_CONTINUATIONS[\"inverse\"].format(subj=subj_plural),\n",
    "            }\n",
    "\n",
    "        elif base[\"language\"] == \"zh\":\n",
    "            cl = ZH_CLASSIFIER.get(base[\"subj\"], \"个\")\n",
    "            cont_map = {\n",
    "                \"surface\": ZH_CONTINUATIONS[\"surface\"].format(cl=cl, subj=base[\"subj\"]),\n",
    "                \"inverse\": ZH_CONTINUATIONS[\"inverse\"].format(cl=cl, subj=base[\"subj\"]),\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown language: {base['language']}\")\n",
    "\n",
    "        for cont_type, cont_text in cont_map.items():\n",
    "            ex = dict(base)\n",
    "            ex[\"continuation_type\"] = cont_type            # \"surface\" or \"inverse\"\n",
    "            ex[\"continuation_text\"] = cont_text            # the thing you'll score\n",
    "            ex[\"full_text\"] = base[\"sentence\"] + cont_text # convenient for debugging\n",
    "            rows.append(ex)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217600e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cont = add_continuations(continuation_df)\n",
    "\n",
    "if \"concept_id\" not in df_cont.columns:\n",
    "    concept_series = df_cont[\"subj\"] + \"|\" + df_cont[\"obj\"] + \"|\" + df_cont[\"verb\"]\n",
    "    df_cont.insert(1, \"concept_id\", concept_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d6994fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote stimuli_with_continuations.csv and stimuli_with_continuations.jsonl\n"
     ]
    }
   ],
   "source": [
    "df_cont.to_csv(os.path.join(STIMULI_DIR, \"stimuli_with_continuations.csv\"), index=False)\n",
    "\n",
    "with open(os.path.join(STIMULI_DIR, \"stimuli_with_continuations.jsonl\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    for row in df_cont.to_dict(orient=\"records\"):\n",
    "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Wrote stimuli_with_continuations.csv and stimuli_with_continuations.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7115d99",
   "metadata": {},
   "source": [
    "## Step 2: Evaluate Log Probs of Continuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb7f0a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyleng/py_venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e639abf",
   "metadata": {},
   "source": [
    "### Load in Continuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f139f006",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSONL_PATH = \"stimuli/stimuli_with_continuations.jsonl\"\n",
    "\n",
    "def read_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                yield json.loads(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f29998aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = list(read_jsonl(JSONL_PATH))\n",
    "continuation_df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c05e25",
   "metadata": {},
   "source": [
    "### Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a52ea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt2\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.float16 if (DEVICE == \"cuda\") else torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1889b2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d341895",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_CSV = \"logprob_surface_vs_inverse.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b7e0983",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a26a9251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=DTYPE\n",
    ").to(DEVICE)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae5f1d3",
   "metadata": {},
   "source": [
    "### Calculate Continuation Log Prob\n",
    "1) Tokenize Prompt and Continuation\n",
    "\n",
    "2) Calculate Log Probs for prompt + continuation\n",
    "\n",
    "3) Calculate continuation log probs: [a] get log prob of each next token from [0,T-1]; [b] sum log probs\n",
    "- Given a prompt of length T, log_prob[:, :T-1, :] gives the probability of the first [0,T-1] tokens\n",
    "\n",
    "```\n",
    "\n",
    "a shark ate every pirate\n",
    "                    ^ stop here; we already know the probability of this word\n",
    "```\n",
    "\n",
    "```python\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        target_log_probs[b,t] = shifted_log_probs[b, t, target_tokens[b,t]]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ba1b175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_probs(prompts: list[str], continuations: list[str]) -> list[dict]:\n",
    "    # 1) Tokenize Prompt and Continuation\n",
    "    enc_base_prompts = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "\n",
    "        # Handle different length prompts\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "\n",
    "        # Avoid [EOS]/[BOS] from being inserted\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "\n",
    "    full_prompts = [p + c for p, c in zip(prompts, continuations)]\n",
    "    enc_full_prompts = tokenizer(\n",
    "        full_prompts,\n",
    "        return_tensors=\"pt\",\n",
    "\n",
    "        # Handle different length prompts\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "\n",
    "        # Avoid [EOS]/[BOS] from being inserted\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "\n",
    "    input_ids = enc_full_prompts[\"input_ids\"].to(DEVICE)\n",
    "    attention_mask = enc_full_prompts[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "    # 2) Calculate logProbs for prompt + continuation\n",
    "    out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = out.logits  # [B, T, V]\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)  # [B, T, V]\n",
    "\n",
    "    # 3) Calculate continuation log probs: [a] get log prob of each next token from [0,T-1]; [b] sum log probs\n",
    "    target_tokens = input_ids[:, 1:]  # [B, T-1]\n",
    "    shifted_log_probs = log_probs[:, :-1, :]  # [B, T-1, V]\n",
    "\n",
    "    # Select the logProb for the selected token\n",
    "    target_log_probs = torch.gather(\n",
    "        input=shifted_log_probs,\n",
    "        dim=-1,  # Select the log_prob for the selected prompt token in vocab\n",
    "        index=target_tokens.unsqueeze(-1)  # [B, T-1, 1]\n",
    "    ).squeeze(-1)  # [B, T-1]\n",
    "\n",
    "    base_prompt_lens = enc_base_prompts[\"attention_mask\"].sum(dim=1).tolist()\n",
    "    full_prompt_lens = enc_full_prompts[\"attention_mask\"].sum(dim=1).tolist()\n",
    "\n",
    "    # The logProbs for the continuation live at (inclusive)\n",
    "    # logits[m-1:n-2] -> logits[m-1:L-2] -> target_log_probs[m-1:L-1] since we already removed one token in the shift\n",
    "    B, _ = target_log_probs.shape\n",
    "\n",
    "    cont_log_probs_list = []\n",
    "\n",
    "    for b in range(B):\n",
    "        base_prompt_length = base_prompt_lens[b]\n",
    "        full_prompt_length = full_prompt_lens[b]\n",
    "\n",
    "        cont_log_probs = target_log_probs[b,\n",
    "                                          base_prompt_length-1:full_prompt_length-1]\n",
    "        cont_log_probs_sum = cont_log_probs.sum().item()\n",
    "        cont_log_probs_mean = cont_log_probs.mean().item()\n",
    "        n_cont_tokens = full_prompt_length - base_prompt_length  # Sanity Check\n",
    "\n",
    "        cont_log_probs_list.append(\n",
    "            {\"cont_log_probs_sum\": cont_log_probs_sum,\n",
    "                \"cont_log_probs_mean\": cont_log_probs_mean,\n",
    "                \"n_cont_tokens\": n_cont_tokens\n",
    "             }\n",
    "        )\n",
    "        # debugging\n",
    "        # print(base_prompt_length, full_prompt_length)\n",
    "        # print(attention_mask[b].shape, attention_mask[b])\n",
    "        # print(target_log_probs[b].shape, target_log_probs[b])\n",
    "        # print(cont_log_probs.shape,cont_log_probs)\n",
    "        # print(\"=\"*10)\n",
    "    return cont_log_probs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a7c57b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_log_probs(df: pd.DataFrame, b_sz: int = BATCH_SIZE):\n",
    "    df = df.reset_index(drop=True).copy() # Pandas safety reasons\n",
    "    all_stats = []\n",
    "\n",
    "    for start in range(0, len(df), b_sz):\n",
    "        end = min(start + b_sz, len(df))\n",
    "        batch = continuation_df.iloc[start:end]\n",
    "\n",
    "        stats = get_log_probs(\n",
    "            prompts=batch[\"sentence\"].tolist(),\n",
    "            continuations=batch[\"continuation_text\"].tolist()\n",
    "        )\n",
    "\n",
    "        all_stats += stats\n",
    "\n",
    "    stats_df = pd.DataFrame(all_stats)    \n",
    "\n",
    "    return pd.concat([df, stats_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5dae38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI Generated pivot table magic to get the metrics\n",
    "def collapse_surface_inverse(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df[df[\"continuation_type\"].isin([\"surface\", \"inverse\"])].copy()\n",
    "\n",
    "    num_cols  = [\"cont_log_probs_sum\", \"cont_log_probs_mean\", \"n_cont_tokens\"]\n",
    "    text_cols = [\"continuation_text\", \"full_text\"]\n",
    "\n",
    "    # metadata that is shared across types\n",
    "    meta_cols = [\n",
    "        c for c in df.columns\n",
    "        if c not in ([\"continuation_type\"] + num_cols + text_cols)\n",
    "    ]\n",
    "    meta = df.groupby(\"stimulus_id\", as_index=False)[meta_cols].first()\n",
    "\n",
    "    # --- 1) pivot numeric columns (keeps float/int dtypes) ---\n",
    "    wide_num = (\n",
    "        df[[\"stimulus_id\", \"continuation_type\"] + num_cols]\n",
    "        .pivot(index=\"stimulus_id\", columns=\"continuation_type\", values=num_cols)\n",
    "    )\n",
    "    wide_num.columns = [f\"{col}_{ctype}\" for col, ctype in wide_num.columns]\n",
    "    wide_num = wide_num.reset_index()\n",
    "\n",
    "    # --- 2) pivot text columns (object dtype is fine here) ---\n",
    "    wide_text = (\n",
    "        df[[\"stimulus_id\", \"continuation_type\"] + text_cols]\n",
    "        .pivot(index=\"stimulus_id\", columns=\"continuation_type\", values=text_cols)\n",
    "    )\n",
    "    wide_text.columns = [f\"{col}_{ctype}\" for col, ctype in wide_text.columns]\n",
    "    wide_text = wide_text.reset_index()\n",
    "\n",
    "    # merge everything\n",
    "    out = meta.merge(wide_num, on=\"stimulus_id\", how=\"left\") \\\n",
    "              .merge(wide_text, on=\"stimulus_id\", how=\"left\")\n",
    "\n",
    "    # deltas + ratios (now safe: numeric dtypes)\n",
    "    out[\"delta_sum\"]  = out[\"cont_log_probs_sum_inverse\"]  - out[\"cont_log_probs_sum_surface\"]\n",
    "    out[\"delta_mean\"] = out[\"cont_log_probs_mean_inverse\"] - out[\"cont_log_probs_mean_surface\"]\n",
    "    out[\"ratio_sum\"]  = np.exp(out[\"delta_sum\"])\n",
    "    out[\"ratio_mean\"] = np.exp(out[\"delta_mean\"])\n",
    "\n",
    "    out[\"preference_mean\"] = np.select(\n",
    "        [out[\"delta_mean\"] > 0, out[\"delta_mean\"] < 0],\n",
    "        [\"inverse\", \"surface\"],\n",
    "        default=\"tie\"\n",
    "    )\n",
    "    out[\"preference_sum\"] = np.select(\n",
    "        [out[\"delta_sum\"] > 0, out[\"delta_sum\"] < 0],\n",
    "        [\"inverse\", \"surface\"],\n",
    "        default=\"tie\"\n",
    "    )\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d694d592",
   "metadata": {},
   "source": [
    "### Save Outputs\n",
    "- Add model name as a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5cda37e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "RESULTS_DIR = Path(\"./results\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b7e51e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "safe_model = re.sub(r\"[^\\w\\-\\.]\", \"_\", MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "29d6b81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_long = process_log_probs(continuation_df, b_sz=BATCH_SIZE)\n",
    "scored_long.insert(0,\"model\", safe_model)\n",
    "\n",
    "scored_wide = collapse_surface_inverse(scored_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b64c6e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>stimulus_id</th>\n",
       "      <th>concept_id</th>\n",
       "      <th>language</th>\n",
       "      <th>template_id</th>\n",
       "      <th>subj</th>\n",
       "      <th>obj</th>\n",
       "      <th>verb</th>\n",
       "      <th>sentence</th>\n",
       "      <th>cont_log_probs_sum_inverse</th>\n",
       "      <th>...</th>\n",
       "      <th>continuation_text_inverse</th>\n",
       "      <th>continuation_text_surface</th>\n",
       "      <th>full_text_inverse</th>\n",
       "      <th>full_text_surface</th>\n",
       "      <th>delta_sum</th>\n",
       "      <th>delta_mean</th>\n",
       "      <th>ratio_sum</th>\n",
       "      <th>ratio_mean</th>\n",
       "      <th>preference_mean</th>\n",
       "      <th>preference_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>en-en_0-000000</td>\n",
       "      <td>shark|pirate|ate</td>\n",
       "      <td>en</td>\n",
       "      <td>en_0</td>\n",
       "      <td>shark</td>\n",
       "      <td>pirate</td>\n",
       "      <td>ate</td>\n",
       "      <td>A shark ate every pirate.</td>\n",
       "      <td>-16.671429</td>\n",
       "      <td>...</td>\n",
       "      <td>There were many sharks.</td>\n",
       "      <td>There was only one shark.</td>\n",
       "      <td>A shark ate every pirate. There were many sharks.</td>\n",
       "      <td>A shark ate every pirate. There was only one s...</td>\n",
       "      <td>-1.176224</td>\n",
       "      <td>-0.751752</td>\n",
       "      <td>0.308441</td>\n",
       "      <td>0.471540</td>\n",
       "      <td>surface</td>\n",
       "      <td>surface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>en-en_0-000001</td>\n",
       "      <td>shark|pirate|helped</td>\n",
       "      <td>en</td>\n",
       "      <td>en_0</td>\n",
       "      <td>shark</td>\n",
       "      <td>pirate</td>\n",
       "      <td>helped</td>\n",
       "      <td>A shark helped every pirate.</td>\n",
       "      <td>-16.237549</td>\n",
       "      <td>...</td>\n",
       "      <td>There were many sharks.</td>\n",
       "      <td>There was only one shark.</td>\n",
       "      <td>A shark helped every pirate. There were many s...</td>\n",
       "      <td>A shark helped every pirate. There was only on...</td>\n",
       "      <td>-0.826022</td>\n",
       "      <td>-0.678922</td>\n",
       "      <td>0.437787</td>\n",
       "      <td>0.507163</td>\n",
       "      <td>surface</td>\n",
       "      <td>surface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>en-en_0-000002</td>\n",
       "      <td>shark|pirate|pushed</td>\n",
       "      <td>en</td>\n",
       "      <td>en_0</td>\n",
       "      <td>shark</td>\n",
       "      <td>pirate</td>\n",
       "      <td>pushed</td>\n",
       "      <td>A shark pushed every pirate.</td>\n",
       "      <td>-16.001699</td>\n",
       "      <td>...</td>\n",
       "      <td>There were many sharks.</td>\n",
       "      <td>There was only one shark.</td>\n",
       "      <td>A shark pushed every pirate. There were many s...</td>\n",
       "      <td>A shark pushed every pirate. There was only on...</td>\n",
       "      <td>-0.633678</td>\n",
       "      <td>-0.639003</td>\n",
       "      <td>0.530636</td>\n",
       "      <td>0.527818</td>\n",
       "      <td>surface</td>\n",
       "      <td>surface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>en-en_0-000003</td>\n",
       "      <td>shark|pirate|chased</td>\n",
       "      <td>en</td>\n",
       "      <td>en_0</td>\n",
       "      <td>shark</td>\n",
       "      <td>pirate</td>\n",
       "      <td>chased</td>\n",
       "      <td>A shark chased every pirate.</td>\n",
       "      <td>-15.284822</td>\n",
       "      <td>...</td>\n",
       "      <td>There were many sharks.</td>\n",
       "      <td>There was only one shark.</td>\n",
       "      <td>A shark chased every pirate. There were many s...</td>\n",
       "      <td>A shark chased every pirate. There was only on...</td>\n",
       "      <td>-0.457541</td>\n",
       "      <td>-0.585751</td>\n",
       "      <td>0.632838</td>\n",
       "      <td>0.556688</td>\n",
       "      <td>surface</td>\n",
       "      <td>surface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>en-en_0-000004</td>\n",
       "      <td>shark|student|ate</td>\n",
       "      <td>en</td>\n",
       "      <td>en_0</td>\n",
       "      <td>shark</td>\n",
       "      <td>student</td>\n",
       "      <td>ate</td>\n",
       "      <td>A shark ate every student.</td>\n",
       "      <td>-16.757080</td>\n",
       "      <td>...</td>\n",
       "      <td>There were many sharks.</td>\n",
       "      <td>There was only one shark.</td>\n",
       "      <td>A shark ate every student. There were many sha...</td>\n",
       "      <td>A shark ate every student. There was only one ...</td>\n",
       "      <td>-1.976831</td>\n",
       "      <td>-0.888041</td>\n",
       "      <td>0.138507</td>\n",
       "      <td>0.411461</td>\n",
       "      <td>surface</td>\n",
       "      <td>surface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>zh-zh_0-000123</td>\n",
       "      <td>狗|医生|追</td>\n",
       "      <td>zh</td>\n",
       "      <td>zh_0</td>\n",
       "      <td>狗</td>\n",
       "      <td>医生</td>\n",
       "      <td>追</td>\n",
       "      <td>有一只狗追了每个医生。</td>\n",
       "      <td>-32.516186</td>\n",
       "      <td>...</td>\n",
       "      <td>有很多只狗。</td>\n",
       "      <td>只有一只狗。</td>\n",
       "      <td>有一只狗追了每个医生。 有很多只狗。</td>\n",
       "      <td>有一只狗追了每个医生。 只有一只狗。</td>\n",
       "      <td>-12.770380</td>\n",
       "      <td>-1.160944</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.313191</td>\n",
       "      <td>surface</td>\n",
       "      <td>surface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>zh-zh_0-000124</td>\n",
       "      <td>狗|游客|吃</td>\n",
       "      <td>zh</td>\n",
       "      <td>zh_0</td>\n",
       "      <td>狗</td>\n",
       "      <td>游客</td>\n",
       "      <td>吃</td>\n",
       "      <td>有一只狗吃了每个游客。</td>\n",
       "      <td>-32.146820</td>\n",
       "      <td>...</td>\n",
       "      <td>有很多只狗。</td>\n",
       "      <td>只有一只狗。</td>\n",
       "      <td>有一只狗吃了每个游客。 有很多只狗。</td>\n",
       "      <td>有一只狗吃了每个游客。 只有一只狗。</td>\n",
       "      <td>-13.137228</td>\n",
       "      <td>-1.194293</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.302918</td>\n",
       "      <td>surface</td>\n",
       "      <td>surface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>zh-zh_0-000125</td>\n",
       "      <td>狗|游客|帮助</td>\n",
       "      <td>zh</td>\n",
       "      <td>zh_0</td>\n",
       "      <td>狗</td>\n",
       "      <td>游客</td>\n",
       "      <td>帮助</td>\n",
       "      <td>有一只狗帮助了每个游客。</td>\n",
       "      <td>-33.794079</td>\n",
       "      <td>...</td>\n",
       "      <td>有很多只狗。</td>\n",
       "      <td>只有一只狗。</td>\n",
       "      <td>有一只狗帮助了每个游客。 有很多只狗。</td>\n",
       "      <td>有一只狗帮助了每个游客。 只有一只狗。</td>\n",
       "      <td>-11.615507</td>\n",
       "      <td>-1.055955</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.347860</td>\n",
       "      <td>surface</td>\n",
       "      <td>surface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>zh-zh_0-000126</td>\n",
       "      <td>狗|游客|推</td>\n",
       "      <td>zh</td>\n",
       "      <td>zh_0</td>\n",
       "      <td>狗</td>\n",
       "      <td>游客</td>\n",
       "      <td>推</td>\n",
       "      <td>有一只狗推了每个游客。</td>\n",
       "      <td>-31.862911</td>\n",
       "      <td>...</td>\n",
       "      <td>有很多只狗。</td>\n",
       "      <td>只有一只狗。</td>\n",
       "      <td>有一只狗推了每个游客。 有很多只狗。</td>\n",
       "      <td>有一只狗推了每个游客。 只有一只狗。</td>\n",
       "      <td>-12.681486</td>\n",
       "      <td>-1.152862</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.315732</td>\n",
       "      <td>surface</td>\n",
       "      <td>surface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>zh-zh_0-000127</td>\n",
       "      <td>狗|游客|追</td>\n",
       "      <td>zh</td>\n",
       "      <td>zh_0</td>\n",
       "      <td>狗</td>\n",
       "      <td>游客</td>\n",
       "      <td>追</td>\n",
       "      <td>有一只狗追了每个游客。</td>\n",
       "      <td>-32.642941</td>\n",
       "      <td>...</td>\n",
       "      <td>有很多只狗。</td>\n",
       "      <td>只有一只狗。</td>\n",
       "      <td>有一只狗追了每个游客。 有很多只狗。</td>\n",
       "      <td>有一只狗追了每个游客。 只有一只狗。</td>\n",
       "      <td>-12.937635</td>\n",
       "      <td>-1.176149</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.308464</td>\n",
       "      <td>surface</td>\n",
       "      <td>surface</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    model     stimulus_id           concept_id language template_id   subj  \\\n",
       "0    gpt2  en-en_0-000000     shark|pirate|ate       en        en_0  shark   \n",
       "1    gpt2  en-en_0-000001  shark|pirate|helped       en        en_0  shark   \n",
       "2    gpt2  en-en_0-000002  shark|pirate|pushed       en        en_0  shark   \n",
       "3    gpt2  en-en_0-000003  shark|pirate|chased       en        en_0  shark   \n",
       "4    gpt2  en-en_0-000004    shark|student|ate       en        en_0  shark   \n",
       "..    ...             ...                  ...      ...         ...    ...   \n",
       "123  gpt2  zh-zh_0-000123               狗|医生|追       zh        zh_0      狗   \n",
       "124  gpt2  zh-zh_0-000124               狗|游客|吃       zh        zh_0      狗   \n",
       "125  gpt2  zh-zh_0-000125              狗|游客|帮助       zh        zh_0      狗   \n",
       "126  gpt2  zh-zh_0-000126               狗|游客|推       zh        zh_0      狗   \n",
       "127  gpt2  zh-zh_0-000127               狗|游客|追       zh        zh_0      狗   \n",
       "\n",
       "         obj    verb                      sentence  \\\n",
       "0     pirate     ate     A shark ate every pirate.   \n",
       "1     pirate  helped  A shark helped every pirate.   \n",
       "2     pirate  pushed  A shark pushed every pirate.   \n",
       "3     pirate  chased  A shark chased every pirate.   \n",
       "4    student     ate    A shark ate every student.   \n",
       "..       ...     ...                           ...   \n",
       "123       医生       追                   有一只狗追了每个医生。   \n",
       "124       游客       吃                   有一只狗吃了每个游客。   \n",
       "125       游客      帮助                  有一只狗帮助了每个游客。   \n",
       "126       游客       推                   有一只狗推了每个游客。   \n",
       "127       游客       追                   有一只狗追了每个游客。   \n",
       "\n",
       "     cont_log_probs_sum_inverse  ...  continuation_text_inverse  \\\n",
       "0                    -16.671429  ...    There were many sharks.   \n",
       "1                    -16.237549  ...    There were many sharks.   \n",
       "2                    -16.001699  ...    There were many sharks.   \n",
       "3                    -15.284822  ...    There were many sharks.   \n",
       "4                    -16.757080  ...    There were many sharks.   \n",
       "..                          ...  ...                        ...   \n",
       "123                  -32.516186  ...                     有很多只狗。   \n",
       "124                  -32.146820  ...                     有很多只狗。   \n",
       "125                  -33.794079  ...                     有很多只狗。   \n",
       "126                  -31.862911  ...                     有很多只狗。   \n",
       "127                  -32.642941  ...                     有很多只狗。   \n",
       "\n",
       "      continuation_text_surface  \\\n",
       "0     There was only one shark.   \n",
       "1     There was only one shark.   \n",
       "2     There was only one shark.   \n",
       "3     There was only one shark.   \n",
       "4     There was only one shark.   \n",
       "..                          ...   \n",
       "123                      只有一只狗。   \n",
       "124                      只有一只狗。   \n",
       "125                      只有一只狗。   \n",
       "126                      只有一只狗。   \n",
       "127                      只有一只狗。   \n",
       "\n",
       "                                     full_text_inverse  \\\n",
       "0    A shark ate every pirate. There were many sharks.   \n",
       "1    A shark helped every pirate. There were many s...   \n",
       "2    A shark pushed every pirate. There were many s...   \n",
       "3    A shark chased every pirate. There were many s...   \n",
       "4    A shark ate every student. There were many sha...   \n",
       "..                                                 ...   \n",
       "123                                 有一只狗追了每个医生。 有很多只狗。   \n",
       "124                                 有一只狗吃了每个游客。 有很多只狗。   \n",
       "125                                有一只狗帮助了每个游客。 有很多只狗。   \n",
       "126                                 有一只狗推了每个游客。 有很多只狗。   \n",
       "127                                 有一只狗追了每个游客。 有很多只狗。   \n",
       "\n",
       "                                     full_text_surface  delta_sum delta_mean  \\\n",
       "0    A shark ate every pirate. There was only one s...  -1.176224  -0.751752   \n",
       "1    A shark helped every pirate. There was only on...  -0.826022  -0.678922   \n",
       "2    A shark pushed every pirate. There was only on...  -0.633678  -0.639003   \n",
       "3    A shark chased every pirate. There was only on...  -0.457541  -0.585751   \n",
       "4    A shark ate every student. There was only one ...  -1.976831  -0.888041   \n",
       "..                                                 ...        ...        ...   \n",
       "123                                 有一只狗追了每个医生。 只有一只狗。 -12.770380  -1.160944   \n",
       "124                                 有一只狗吃了每个游客。 只有一只狗。 -13.137228  -1.194293   \n",
       "125                                有一只狗帮助了每个游客。 只有一只狗。 -11.615507  -1.055955   \n",
       "126                                 有一只狗推了每个游客。 只有一只狗。 -12.681486  -1.152862   \n",
       "127                                 有一只狗追了每个游客。 只有一只狗。 -12.937635  -1.176149   \n",
       "\n",
       "    ratio_sum ratio_mean preference_mean  preference_sum  \n",
       "0    0.308441   0.471540         surface         surface  \n",
       "1    0.437787   0.507163         surface         surface  \n",
       "2    0.530636   0.527818         surface         surface  \n",
       "3    0.632838   0.556688         surface         surface  \n",
       "4    0.138507   0.411461         surface         surface  \n",
       "..        ...        ...             ...             ...  \n",
       "123  0.000003   0.313191         surface         surface  \n",
       "124  0.000002   0.302918         surface         surface  \n",
       "125  0.000009   0.347860         surface         surface  \n",
       "126  0.000003   0.315732         surface         surface  \n",
       "127  0.000002   0.308464         surface         surface  \n",
       "\n",
       "[128 rows x 25 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scored_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f4e4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_long.to_json(\n",
    "    RESULTS_DIR / f\"scored_long_{safe_model}.jsonl\",\n",
    "    orient=\"records\",\n",
    "    lines=True\n",
    ")\n",
    "\n",
    "scored_wide.to_json(\n",
    "    RESULTS_DIR / f\"scored_wide_{safe_model}.jsonl\",\n",
    "    orient=\"records\",\n",
    "    lines=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
