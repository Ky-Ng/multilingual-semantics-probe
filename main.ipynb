{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61b6f226",
   "metadata": {},
   "source": [
    "# Multilingual Semantics Probe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ea80eb",
   "metadata": {},
   "source": [
    "## Step 1: Corpus Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "c9961ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "1c6656ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "STIMULI_DIR = \"./stimuli\"\n",
    "EXISTENTIAL_UNIVERSAL = True\n",
    "quantifier_type_str = \"eu\" if EXISTENTIAL_UNIVERSAL else \"ue\"\n",
    "\n",
    "if not os.path.exists(STIMULI_DIR):\n",
    "    os.mkdir(STIMULI_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "e5aba402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- English lexicon ---\n",
    "EN_SUBJECTS = [\n",
    "    \"shark\",\n",
    "    \"robot\",\n",
    "    \"chef\",\n",
    "    \"dog\",\n",
    "    \"kangaroo\",\n",
    "]\n",
    "\n",
    "EN_OBJECTS = [\n",
    "    \"pirate\",\n",
    "    \"student\",\n",
    "    \"doctor\",\n",
    "    \"tourist\",\n",
    "    \"politician\"\n",
    "]\n",
    "\n",
    "# Use correct simple past forms\n",
    "EN_VERBS_PAST = [\n",
    "    \"ate\",\n",
    "    \"helped\",\n",
    "    \"pushed\",\n",
    "    \"chased\",\n",
    "]\n",
    "\n",
    "# --- Mandarin lexicon ---\n",
    "# Bare nouns only (no quantifiers inside)\n",
    "ZH_SUBJECTS = [\n",
    "    \"鲨鱼\",\n",
    "    \"机器人\",\n",
    "    \"厨师\",\n",
    "    \"狗\",\n",
    "    \"袋鼠\"\n",
    "]\n",
    "\n",
    "ZH_OBJECTS = [\n",
    "    \"海盗\",\n",
    "    \"学生\",\n",
    "    \"医生\",\n",
    "    \"游客\",\n",
    "    \"政治家\"\n",
    "]\n",
    "\n",
    "# Verb stems compatible with 了\n",
    "ZH_VERBS = [\n",
    "    \"吃\",\n",
    "    \"帮助\",\n",
    "    \"推\",\n",
    "    \"追\",\n",
    "]\n",
    "\n",
    "# Optional classifier map (defaults to 个)\n",
    "ZH_CLASSIFIER: Dict[str, str] = {\n",
    "    \"鲨鱼\": \"只\",\n",
    "    \"狗\": \"只\",\n",
    "    \"机器人\": \"个\",\n",
    "    \"厨师\": \"个\",\n",
    "    \"海盗\": \"个\",\n",
    "    \"学生\": \"个\",\n",
    "    \"医生\": \"个\",\n",
    "    \"游客\": \"个\",\n",
    "    \"袋鼠\": \"只\",\n",
    "    \"政治家\": \"个\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "44a60565",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXISTENTIAL_UNIVERSAL:\n",
    "    EN_TEMPLATES = [\n",
    "        # Classic ambiguous English form\n",
    "        \"A {subj} {verb_past} every {obj}.\",\n",
    "    ]\n",
    "\n",
    "    ZH_TEMPLATES = [\n",
    "        # Canonical Mandarin surface-scope reading\n",
    "        \"有一{cl}{subj}{verb}了每个{obj}。\",\n",
    "    ]\n",
    "else:\n",
    "    EN_TEMPLATES = [\n",
    "        # Classic ambiguous English form\n",
    "        \"Every {subj} {verb_past} a {obj}.\",\n",
    "    ]\n",
    "\n",
    "    ZH_TEMPLATES = [\n",
    "        # Universal-Existential Construction\n",
    "        \"每个{cl}{subj}{verb}了一个{obj}。\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "2202da3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Stimulus:\n",
    "    language: str\n",
    "    template_id: str\n",
    "    subj: str\n",
    "    obj: str\n",
    "    verb: str\n",
    "    sentence: str\n",
    "\n",
    "    # Unique cross linguistic identifiers\n",
    "    tid: int\n",
    "    subj_i: int\n",
    "    obj_i: int\n",
    "    verb_i: int\n",
    "\n",
    "\n",
    "\n",
    "def get_classifier(noun: str, cl_map: Dict[str, str]) -> str:\n",
    "    return cl_map.get(noun, \"个\")\n",
    "\n",
    "\n",
    "def generate_english(\n",
    "    subjects: List[str],\n",
    "    objects: List[str],\n",
    "    verbs_past: List[str],\n",
    ") -> List[Stimulus]:\n",
    "    out: List[Stimulus] = []\n",
    "    for tid, tmpl in enumerate(EN_TEMPLATES):\n",
    "        for subj_i, subj in enumerate(subjects):\n",
    "            for obj_i, obj in enumerate(objects):\n",
    "                for verb_i, verb in enumerate(verbs_past):\n",
    "                    out.append(\n",
    "                        Stimulus(\n",
    "                            language=\"en\",\n",
    "                            template_id=f\"en_{tid}\",\n",
    "                            subj=subj,\n",
    "                            obj=obj,\n",
    "                            verb=verb,\n",
    "                            sentence=tmpl.format(\n",
    "                                subj=subj,\n",
    "                                obj=obj,\n",
    "                                verb_past=verb,\n",
    "                            ),\n",
    "                            tid=tid,\n",
    "                            subj_i=subj_i,\n",
    "                            obj_i=obj_i,\n",
    "                            verb_i=verb_i,\n",
    "                        )\n",
    "                    )\n",
    "    return out\n",
    "\n",
    "def generate_mandarin(\n",
    "    subjects: List[str],\n",
    "    objects: List[str],\n",
    "    verbs: List[str],\n",
    "    cl_map: Dict[str, str],\n",
    ") -> List[Stimulus]:\n",
    "    out: List[Stimulus] = []\n",
    "    for tid, tmpl in enumerate(ZH_TEMPLATES):\n",
    "        for subj_i, subj in enumerate(subjects):\n",
    "            for obj_i, obj in enumerate(objects):\n",
    "                for verb_i, verb in enumerate(verbs):\n",
    "                    cl = get_classifier(subj, cl_map)\n",
    "                    out.append(\n",
    "                        Stimulus(\n",
    "                            language=\"zh\",\n",
    "                            template_id=f\"zh_{tid}\",\n",
    "                            subj=subj,\n",
    "                            obj=obj,\n",
    "                            verb=verb,\n",
    "                            sentence=tmpl.format(\n",
    "                                cl=cl,\n",
    "                                subj=subj,\n",
    "                                obj=obj,\n",
    "                                verb=verb,\n",
    "                            ),\n",
    "                            tid=tid,\n",
    "                            subj_i=subj_i,\n",
    "                            obj_i=obj_i,\n",
    "                            verb_i=verb_i,\n",
    "                        )\n",
    "                    )\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "d6e34c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stimuli = []\n",
    "stimuli += generate_english(EN_SUBJECTS, EN_OBJECTS, EN_VERBS_PAST)\n",
    "stimuli += generate_mandarin(ZH_SUBJECTS, ZH_OBJECTS, ZH_VERBS, ZH_CLASSIFIER)\n",
    "\n",
    "continuation_df = pd.DataFrame([s.__dict__ for s in stimuli])\n",
    "\n",
    "# Language-invariant semantic ID\n",
    "continuation_df[\"pair_id\"] = continuation_df.apply(\n",
    "    lambda r: f\"t{r.tid}_s{r.subj_i}_o{r.obj_i}_v{r.verb_i}\",\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Final unique ID\n",
    "continuation_df.insert(\n",
    "    0,\n",
    "    \"stimulus_id\",\n",
    "    continuation_df.apply(\n",
    "        lambda r: f\"{r.language}-{r.pair_id}\",\n",
    "        axis=1,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "186f7040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total stimuli: 200\n",
      "language\n",
      "en    100\n",
      "zh    100\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stimulus_id</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>en-t0_s1_o1_v2</td>\n",
       "      <td>A robot pushed every student.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>en-t0_s4_o1_v2</td>\n",
       "      <td>A kangaroo pushed every student.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en-t0_s0_o0_v2</td>\n",
       "      <td>A shark pushed every pirate.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>en-t0_s2_o3_v3</td>\n",
       "      <td>A chef chased every tourist.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>en-t0_s3_o3_v3</td>\n",
       "      <td>A dog chased every tourist.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       stimulus_id                          sentence\n",
       "26  en-t0_s1_o1_v2     A robot pushed every student.\n",
       "86  en-t0_s4_o1_v2  A kangaroo pushed every student.\n",
       "2   en-t0_s0_o0_v2      A shark pushed every pirate.\n",
       "55  en-t0_s2_o3_v3      A chef chased every tourist.\n",
       "75  en-t0_s3_o3_v3       A dog chased every tourist."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stimulus_id</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>zh-t0_s1_o1_v2</td>\n",
       "      <td>有一个机器人推了每个学生。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>zh-t0_s4_o1_v2</td>\n",
       "      <td>有一只袋鼠推了每个学生。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>zh-t0_s0_o0_v2</td>\n",
       "      <td>有一只鲨鱼推了每个海盗。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>zh-t0_s2_o3_v3</td>\n",
       "      <td>有一个厨师追了每个游客。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>zh-t0_s3_o3_v3</td>\n",
       "      <td>有一只狗追了每个游客。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        stimulus_id       sentence\n",
       "126  zh-t0_s1_o1_v2  有一个机器人推了每个学生。\n",
       "186  zh-t0_s4_o1_v2   有一只袋鼠推了每个学生。\n",
       "102  zh-t0_s0_o0_v2   有一只鲨鱼推了每个海盗。\n",
       "155  zh-t0_s2_o3_v3   有一个厨师追了每个游客。\n",
       "175  zh-t0_s3_o3_v3    有一只狗追了每个游客。"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Total stimuli:\", len(continuation_df))\n",
    "print(continuation_df[\"language\"].value_counts())\n",
    "\n",
    "display(\n",
    "    continuation_df[continuation_df[\"language\"] == \"en\"][[\"stimulus_id\", \"sentence\"]].sample(\n",
    "        min(5, (continuation_df[\"language\"] == \"en\").sum()),\n",
    "        random_state=0,\n",
    "    )\n",
    ")\n",
    "\n",
    "display(\n",
    "    continuation_df[continuation_df[\"language\"] == \"zh\"][[\"stimulus_id\", \"sentence\"]].sample(\n",
    "        min(5, (continuation_df[\"language\"] == \"zh\").sum()),\n",
    "        random_state=0,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "891de94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote stimuli_eu.csv and stimuli_eu.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Serialize\n",
    "continuation_df.to_csv(os.path.join(STIMULI_DIR,f\"stimuli_{quantifier_type_str}.csv\"), index=False)\n",
    "\n",
    "with open(os.path.join(STIMULI_DIR, f\"stimuli_{quantifier_type_str}.jsonl\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    for row in continuation_df.to_dict(orient=\"records\"):\n",
    "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Wrote stimuli_{quantifier_type_str}.csv and stimuli_{quantifier_type_str}.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79665af",
   "metadata": {},
   "source": [
    "### Add Natural Language Continuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "b9bc3d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_CONTINUATIONS = {\n",
    "    \"surface\": \" There was only one {noun}.\",\n",
    "    \"inverse\": \" There were many {noun}.\",\n",
    "}\n",
    "\n",
    "# Mandarin: keep equally short.\n",
    "# Note: plural is usually implicit; \"很多\" is a decent lexical cue.\n",
    "ZH_CONTINUATIONS = {\n",
    "    \"surface\": \" 只有一{cl}{noun}。\",\n",
    "    \"inverse\": \" 有很多{cl}{noun}。\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "98b3708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_continuations(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for r in df.itertuples(index=False):\n",
    "        base = r._asdict()\n",
    "\n",
    "        if base[\"language\"] == \"en\":\n",
    "            # naive pluralization: add \"s\"\n",
    "            # If you care about irregular plurals later, add a map.\n",
    "            subj_plural = base[\"subj\"] + \"s\"\n",
    "            obj_plural = base[\"obj\"] + \"s\"\n",
    "\n",
    "            cont_map = {\n",
    "                \"surface\": EN_CONTINUATIONS[\"surface\"].format(noun=base[\"subj\"] if EXISTENTIAL_UNIVERSAL else base[\"obj\"]),\n",
    "                \"inverse\": EN_CONTINUATIONS[\"inverse\"].format(noun=subj_plural if EXISTENTIAL_UNIVERSAL else obj_plural),\n",
    "            }\n",
    "\n",
    "        elif base[\"language\"] == \"zh\":\n",
    "            cl = ZH_CLASSIFIER.get(base[\"subj\"], \"个\")\n",
    "            cont_map = {\n",
    "                \"surface\": ZH_CONTINUATIONS[\"surface\"].format(cl=cl, noun=base[\"subj\"] if EXISTENTIAL_UNIVERSAL else base[\"obj\"]),\n",
    "                \"inverse\": ZH_CONTINUATIONS[\"inverse\"].format(cl=cl, noun=base[\"subj\"] if EXISTENTIAL_UNIVERSAL else base[\"obj\"]),\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown language: {base['language']}\")\n",
    "\n",
    "        for cont_type, cont_text in cont_map.items():\n",
    "            ex = dict(base)\n",
    "            ex[\"continuation_type\"] = cont_type            # \"surface\" or \"inverse\"\n",
    "            ex[\"continuation_text\"] = cont_text            # the thing you'll score\n",
    "            ex[\"full_text\"] = base[\"sentence\"] + cont_text # convenient for debugging\n",
    "            rows.append(ex)\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "217600e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cont = add_continuations(continuation_df)\n",
    "\n",
    "if \"concept_id\" not in df_cont.columns:\n",
    "    concept_series = df_cont[\"subj\"] + \"|\" + df_cont[\"obj\"] + \"|\" + df_cont[\"verb\"]\n",
    "    df_cont.insert(1, \"concept_id\", concept_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "d6994fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote stimuli_with_continuations_eu.csv and stimuli_with_continuations_eu.jsonl\n"
     ]
    }
   ],
   "source": [
    "df_cont.to_csv(os.path.join(STIMULI_DIR, f\"stimuli_with_continuations_{quantifier_type_str}.csv\"), index=False)\n",
    "\n",
    "with open(os.path.join(STIMULI_DIR, f\"stimuli_with_continuations_{quantifier_type_str}.jsonl\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    for row in df_cont.to_dict(orient=\"records\"):\n",
    "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Wrote stimuli_with_continuations_{quantifier_type_str}.csv and stimuli_with_continuations_{quantifier_type_str}.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7115d99",
   "metadata": {},
   "source": [
    "## Step 2: Evaluate Log Probs of Continuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7f0a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e639abf",
   "metadata": {},
   "source": [
    "### Load in Continuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "f139f006",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSONL_PATH = f\"stimuli/stimuli_with_continuations_{quantifier_type_str}.jsonl\"\n",
    "\n",
    "def read_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                yield json.loads(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "f29998aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = list(read_jsonl(JSONL_PATH))\n",
    "continuation_df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c05e25",
   "metadata": {},
   "source": [
    "### Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "1a52ea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"uer/gpt2-chinese-cluecorpussmall\"\n",
    "LOWER_CASE = True\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.bfloat16 if (DEVICE == \"cuda\") else torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "1889b2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "0d341895",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_CSV = \"logprob_surface_vs_inverse.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "4b7e0983",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "a26a9251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(21128, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=21128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=DTYPE\n",
    ").to(DEVICE)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae5f1d3",
   "metadata": {},
   "source": [
    "### Calculate Continuation Log Prob\n",
    "1) Tokenize Prompt and Continuation\n",
    "\n",
    "2) Calculate Log Probs for prompt + continuation\n",
    "\n",
    "3) Calculate continuation log probs: [a] get log prob of each next token from [0,T-1]; [b] sum log probs\n",
    "- Given a prompt of length T, log_prob[:, :T-1, :] gives the probability of the first [0,T-1] tokens\n",
    "\n",
    "```\n",
    "\n",
    "a shark ate every pirate\n",
    "                    ^ stop here; we already know the probability of this word\n",
    "```\n",
    "\n",
    "```python\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        target_log_probs[b,t] = shifted_log_probs[b, t, target_tokens[b,t]]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "1ba1b175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_probs(prompts: list[str], continuations: list[str]) -> list[dict]:\n",
    "    # 1) Tokenize Prompt and Continuation\n",
    "    enc_base_prompts = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "\n",
    "        # Handle different length prompts\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "\n",
    "        # Avoid [EOS]/[BOS] from being inserted\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "\n",
    "    full_prompts = [p + c for p, c in zip(prompts, continuations)]\n",
    "    \n",
    "    full_prompts = [p.lower() for p in full_prompts] if LOWER_CASE else full_prompts\n",
    "    \n",
    "    enc_full_prompts = tokenizer(\n",
    "        full_prompts,\n",
    "        return_tensors=\"pt\",\n",
    "\n",
    "        # Handle different length prompts\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "\n",
    "        # Avoid [EOS]/[BOS] from being inserted\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "\n",
    "    input_ids = enc_full_prompts[\"input_ids\"].to(DEVICE)\n",
    "    attention_mask = enc_full_prompts[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "    # 2) Calculate logProbs for prompt + continuation\n",
    "    out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = out.logits.to(torch.float32)  # [B, T, V]\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)  # [B, T, V]\n",
    "\n",
    "    # 3) Calculate continuation log probs: [a] get log prob of each next token from [0,T-1]; [b] sum log probs\n",
    "    target_tokens = input_ids[:, 1:]  # [B, T-1]\n",
    "    shifted_log_probs = log_probs[:, :-1, :]  # [B, T-1, V]\n",
    "\n",
    "    # Select the logProb for the selected token\n",
    "    target_log_probs = torch.gather(\n",
    "        input=shifted_log_probs,\n",
    "        dim=-1,  # Select the log_prob for the selected prompt token in vocab\n",
    "        index=target_tokens.unsqueeze(-1)  # [B, T-1, 1]\n",
    "    ).squeeze(-1)  # [B, T-1]\n",
    "\n",
    "    base_prompt_lens = enc_base_prompts[\"attention_mask\"].sum(dim=1).tolist()\n",
    "    full_prompt_lens = enc_full_prompts[\"attention_mask\"].sum(dim=1).tolist()\n",
    "\n",
    "    # The logProbs for the continuation live at (inclusive)\n",
    "    # logits[m-1:n-2] -> logits[m-1:L-2] -> target_log_probs[m-1:L-1] since we already removed one token in the shift\n",
    "    B, _ = target_log_probs.shape\n",
    "\n",
    "    cont_log_probs_list = []\n",
    "\n",
    "    for b in range(B):\n",
    "        base_prompt_length = base_prompt_lens[b]\n",
    "        full_prompt_length = full_prompt_lens[b]\n",
    "\n",
    "        cont_log_probs = target_log_probs[b,\n",
    "                                          base_prompt_length-1:full_prompt_length-1]\n",
    "        cont_log_probs_sum = cont_log_probs.sum().item()\n",
    "        cont_log_probs_mean = cont_log_probs.mean().item()\n",
    "        n_cont_tokens = full_prompt_length - base_prompt_length  # Sanity Check\n",
    "\n",
    "        cont_log_probs_list.append(\n",
    "            {\"cont_log_probs_sum\": cont_log_probs_sum,\n",
    "                \"cont_log_probs_mean\": cont_log_probs_mean,\n",
    "                \"n_cont_tokens\": n_cont_tokens\n",
    "             }\n",
    "        )\n",
    "    return cont_log_probs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "5a7c57b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_log_probs(df: pd.DataFrame, b_sz: int = BATCH_SIZE):\n",
    "    df = df.reset_index(drop=True).copy() # Pandas safety reasons\n",
    "    all_stats = []\n",
    "\n",
    "    for start in range(0, len(df), b_sz):\n",
    "        end = min(start + b_sz, len(df))\n",
    "        batch = continuation_df.iloc[start:end]\n",
    "\n",
    "        stats = get_log_probs(\n",
    "            prompts=batch[\"sentence\"].tolist(),\n",
    "            continuations=batch[\"continuation_text\"].tolist()\n",
    "        )\n",
    "\n",
    "        all_stats += stats\n",
    "\n",
    "    stats_df = pd.DataFrame(all_stats)    \n",
    "\n",
    "    return pd.concat([df, stats_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "d5dae38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI Generated pivot table magic to get the metrics\n",
    "def collapse_surface_inverse(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df[df[\"continuation_type\"].isin([\"surface\", \"inverse\"])].copy()\n",
    "\n",
    "    num_cols  = [\"cont_log_probs_sum\", \"cont_log_probs_mean\", \"n_cont_tokens\"]\n",
    "    text_cols = [\"continuation_text\", \"full_text\"]\n",
    "\n",
    "    # metadata that is shared across types\n",
    "    meta_cols = [\n",
    "        c for c in df.columns\n",
    "        if c not in ([\"continuation_type\"] + num_cols + text_cols)\n",
    "    ]\n",
    "    meta = df.groupby(\"stimulus_id\", as_index=False)[meta_cols].first()\n",
    "\n",
    "    # --- 1) pivot numeric columns (keeps float/int dtypes) ---\n",
    "    wide_num = (\n",
    "        df[[\"stimulus_id\", \"continuation_type\"] + num_cols]\n",
    "        .pivot(index=\"stimulus_id\", columns=\"continuation_type\", values=num_cols)\n",
    "    )\n",
    "    wide_num.columns = [f\"{col}_{ctype}\" for col, ctype in wide_num.columns]\n",
    "    wide_num = wide_num.reset_index()\n",
    "\n",
    "    # --- 2) pivot text columns (object dtype is fine here) ---\n",
    "    wide_text = (\n",
    "        df[[\"stimulus_id\", \"continuation_type\"] + text_cols]\n",
    "        .pivot(index=\"stimulus_id\", columns=\"continuation_type\", values=text_cols)\n",
    "    )\n",
    "    wide_text.columns = [f\"{col}_{ctype}\" for col, ctype in wide_text.columns]\n",
    "    wide_text = wide_text.reset_index()\n",
    "\n",
    "    # merge everything\n",
    "    out = meta.merge(wide_num, on=\"stimulus_id\", how=\"left\") \\\n",
    "              .merge(wide_text, on=\"stimulus_id\", how=\"left\")\n",
    "\n",
    "    # deltas + ratios (now safe: numeric dtypes)\n",
    "    out[\"delta_sum\"]  = out[\"cont_log_probs_sum_inverse\"]  - out[\"cont_log_probs_sum_surface\"]\n",
    "    out[\"ratio_sum\"]  = np.exp(out[\"delta_sum\"])\n",
    "\n",
    "    out[\"delta_mean\"] = out[\"cont_log_probs_mean_inverse\"] - out[\"cont_log_probs_mean_surface\"]\n",
    "    out[\"ratio_mean\"] = np.exp(out[\"delta_mean\"])\n",
    "\n",
    "    out[\"preference_sum\"] = np.select(\n",
    "        [out[\"delta_sum\"] > 0, out[\"delta_sum\"] < 0],\n",
    "        [\"inverse\", \"surface\"],\n",
    "        default=\"tie\"\n",
    "    )\n",
    "\n",
    "    out[\"preference_mean\"] = np.select(\n",
    "        [out[\"delta_mean\"] > 0, out[\"delta_mean\"] < 0],\n",
    "        [\"inverse\", \"surface\"],\n",
    "        default=\"tie\"\n",
    "    )\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d694d592",
   "metadata": {},
   "source": [
    "### Save Outputs\n",
    "- Add model name as a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "5cda37e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "RESULTS_DIR = Path(\"./results\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "b7e51e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "safe_model = re.sub(r\"[^\\w\\-\\.]\", \"_\", MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "29d6b81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_long = process_log_probs(continuation_df, b_sz=BATCH_SIZE)\n",
    "scored_long.insert(0,\"model\", safe_model)\n",
    "\n",
    "scored_wide = collapse_surface_inverse(scored_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "b64c6e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>stimulus_id</th>\n",
       "      <th>concept_id</th>\n",
       "      <th>language</th>\n",
       "      <th>template_id</th>\n",
       "      <th>subj</th>\n",
       "      <th>obj</th>\n",
       "      <th>verb</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tid</th>\n",
       "      <th>...</th>\n",
       "      <th>continuation_text_inverse</th>\n",
       "      <th>continuation_text_surface</th>\n",
       "      <th>full_text_inverse</th>\n",
       "      <th>full_text_surface</th>\n",
       "      <th>delta_sum</th>\n",
       "      <th>ratio_sum</th>\n",
       "      <th>delta_mean</th>\n",
       "      <th>ratio_mean</th>\n",
       "      <th>preference_sum</th>\n",
       "      <th>preference_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uer_gpt2-chinese-cluecorpussmall</td>\n",
       "      <td>en-t0_s0_o0_v0</td>\n",
       "      <td>shark|pirate|ate</td>\n",
       "      <td>en</td>\n",
       "      <td>en_0</td>\n",
       "      <td>shark</td>\n",
       "      <td>pirate</td>\n",
       "      <td>ate</td>\n",
       "      <td>A shark ate every pirate.</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>There were many sharks.</td>\n",
       "      <td>There was only one shark.</td>\n",
       "      <td>A shark ate every pirate. There were many sharks.</td>\n",
       "      <td>A shark ate every pirate. There was only one s...</td>\n",
       "      <td>2.095390</td>\n",
       "      <td>8.128613</td>\n",
       "      <td>1.069577</td>\n",
       "      <td>2.914146</td>\n",
       "      <td>inverse</td>\n",
       "      <td>inverse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uer_gpt2-chinese-cluecorpussmall</td>\n",
       "      <td>en-t0_s0_o0_v1</td>\n",
       "      <td>shark|pirate|helped</td>\n",
       "      <td>en</td>\n",
       "      <td>en_0</td>\n",
       "      <td>shark</td>\n",
       "      <td>pirate</td>\n",
       "      <td>helped</td>\n",
       "      <td>A shark helped every pirate.</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>There were many sharks.</td>\n",
       "      <td>There was only one shark.</td>\n",
       "      <td>A shark helped every pirate. There were many s...</td>\n",
       "      <td>A shark helped every pirate. There was only on...</td>\n",
       "      <td>1.271116</td>\n",
       "      <td>3.564830</td>\n",
       "      <td>1.008132</td>\n",
       "      <td>2.740476</td>\n",
       "      <td>inverse</td>\n",
       "      <td>inverse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uer_gpt2-chinese-cluecorpussmall</td>\n",
       "      <td>en-t0_s0_o0_v2</td>\n",
       "      <td>shark|pirate|pushed</td>\n",
       "      <td>en</td>\n",
       "      <td>en_0</td>\n",
       "      <td>shark</td>\n",
       "      <td>pirate</td>\n",
       "      <td>pushed</td>\n",
       "      <td>A shark pushed every pirate.</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>There were many sharks.</td>\n",
       "      <td>There was only one shark.</td>\n",
       "      <td>A shark pushed every pirate. There were many s...</td>\n",
       "      <td>A shark pushed every pirate. There was only on...</td>\n",
       "      <td>1.041332</td>\n",
       "      <td>2.832989</td>\n",
       "      <td>0.953306</td>\n",
       "      <td>2.594273</td>\n",
       "      <td>inverse</td>\n",
       "      <td>inverse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uer_gpt2-chinese-cluecorpussmall</td>\n",
       "      <td>en-t0_s0_o0_v3</td>\n",
       "      <td>shark|pirate|chased</td>\n",
       "      <td>en</td>\n",
       "      <td>en_0</td>\n",
       "      <td>shark</td>\n",
       "      <td>pirate</td>\n",
       "      <td>chased</td>\n",
       "      <td>A shark chased every pirate.</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>There were many sharks.</td>\n",
       "      <td>There was only one shark.</td>\n",
       "      <td>A shark chased every pirate. There were many s...</td>\n",
       "      <td>A shark chased every pirate. There was only on...</td>\n",
       "      <td>1.033434</td>\n",
       "      <td>2.810701</td>\n",
       "      <td>0.961239</td>\n",
       "      <td>2.614933</td>\n",
       "      <td>inverse</td>\n",
       "      <td>inverse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uer_gpt2-chinese-cluecorpussmall</td>\n",
       "      <td>en-t0_s0_o1_v0</td>\n",
       "      <td>shark|student|ate</td>\n",
       "      <td>en</td>\n",
       "      <td>en_0</td>\n",
       "      <td>shark</td>\n",
       "      <td>student</td>\n",
       "      <td>ate</td>\n",
       "      <td>A shark ate every student.</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>There were many sharks.</td>\n",
       "      <td>There was only one shark.</td>\n",
       "      <td>A shark ate every student. There were many sha...</td>\n",
       "      <td>A shark ate every student. There was only one ...</td>\n",
       "      <td>2.197477</td>\n",
       "      <td>9.002275</td>\n",
       "      <td>1.089395</td>\n",
       "      <td>2.972474</td>\n",
       "      <td>inverse</td>\n",
       "      <td>inverse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>uer_gpt2-chinese-cluecorpussmall</td>\n",
       "      <td>zh-t0_s4_o3_v3</td>\n",
       "      <td>袋鼠|游客|追</td>\n",
       "      <td>zh</td>\n",
       "      <td>zh_0</td>\n",
       "      <td>袋鼠</td>\n",
       "      <td>游客</td>\n",
       "      <td>追</td>\n",
       "      <td>有一只袋鼠追了每个游客。</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>有很多只袋鼠。</td>\n",
       "      <td>只有一只袋鼠。</td>\n",
       "      <td>有一只袋鼠追了每个游客。 有很多只袋鼠。</td>\n",
       "      <td>有一只袋鼠追了每个游客。 只有一只袋鼠。</td>\n",
       "      <td>-7.002993</td>\n",
       "      <td>0.000909</td>\n",
       "      <td>-1.000427</td>\n",
       "      <td>0.367722</td>\n",
       "      <td>surface</td>\n",
       "      <td>surface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>uer_gpt2-chinese-cluecorpussmall</td>\n",
       "      <td>zh-t0_s4_o4_v0</td>\n",
       "      <td>袋鼠|政治家|吃</td>\n",
       "      <td>zh</td>\n",
       "      <td>zh_0</td>\n",
       "      <td>袋鼠</td>\n",
       "      <td>政治家</td>\n",
       "      <td>吃</td>\n",
       "      <td>有一只袋鼠吃了每个政治家。</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>有很多只袋鼠。</td>\n",
       "      <td>只有一只袋鼠。</td>\n",
       "      <td>有一只袋鼠吃了每个政治家。 有很多只袋鼠。</td>\n",
       "      <td>有一只袋鼠吃了每个政治家。 只有一只袋鼠。</td>\n",
       "      <td>-5.853102</td>\n",
       "      <td>0.002871</td>\n",
       "      <td>-0.836157</td>\n",
       "      <td>0.433373</td>\n",
       "      <td>surface</td>\n",
       "      <td>surface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>uer_gpt2-chinese-cluecorpussmall</td>\n",
       "      <td>zh-t0_s4_o4_v1</td>\n",
       "      <td>袋鼠|政治家|帮助</td>\n",
       "      <td>zh</td>\n",
       "      <td>zh_0</td>\n",
       "      <td>袋鼠</td>\n",
       "      <td>政治家</td>\n",
       "      <td>帮助</td>\n",
       "      <td>有一只袋鼠帮助了每个政治家。</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>有很多只袋鼠。</td>\n",
       "      <td>只有一只袋鼠。</td>\n",
       "      <td>有一只袋鼠帮助了每个政治家。 有很多只袋鼠。</td>\n",
       "      <td>有一只袋鼠帮助了每个政治家。 只有一只袋鼠。</td>\n",
       "      <td>-5.490833</td>\n",
       "      <td>0.004124</td>\n",
       "      <td>-0.784405</td>\n",
       "      <td>0.456391</td>\n",
       "      <td>surface</td>\n",
       "      <td>surface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>uer_gpt2-chinese-cluecorpussmall</td>\n",
       "      <td>zh-t0_s4_o4_v2</td>\n",
       "      <td>袋鼠|政治家|推</td>\n",
       "      <td>zh</td>\n",
       "      <td>zh_0</td>\n",
       "      <td>袋鼠</td>\n",
       "      <td>政治家</td>\n",
       "      <td>推</td>\n",
       "      <td>有一只袋鼠推了每个政治家。</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>有很多只袋鼠。</td>\n",
       "      <td>只有一只袋鼠。</td>\n",
       "      <td>有一只袋鼠推了每个政治家。 有很多只袋鼠。</td>\n",
       "      <td>有一只袋鼠推了每个政治家。 只有一只袋鼠。</td>\n",
       "      <td>-6.617328</td>\n",
       "      <td>0.001337</td>\n",
       "      <td>-0.945333</td>\n",
       "      <td>0.388550</td>\n",
       "      <td>surface</td>\n",
       "      <td>surface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>uer_gpt2-chinese-cluecorpussmall</td>\n",
       "      <td>zh-t0_s4_o4_v3</td>\n",
       "      <td>袋鼠|政治家|追</td>\n",
       "      <td>zh</td>\n",
       "      <td>zh_0</td>\n",
       "      <td>袋鼠</td>\n",
       "      <td>政治家</td>\n",
       "      <td>追</td>\n",
       "      <td>有一只袋鼠追了每个政治家。</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>有很多只袋鼠。</td>\n",
       "      <td>只有一只袋鼠。</td>\n",
       "      <td>有一只袋鼠追了每个政治家。 有很多只袋鼠。</td>\n",
       "      <td>有一只袋鼠追了每个政治家。 只有一只袋鼠。</td>\n",
       "      <td>-6.748816</td>\n",
       "      <td>0.001172</td>\n",
       "      <td>-0.964117</td>\n",
       "      <td>0.381320</td>\n",
       "      <td>surface</td>\n",
       "      <td>surface</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                model     stimulus_id           concept_id  \\\n",
       "0    uer_gpt2-chinese-cluecorpussmall  en-t0_s0_o0_v0     shark|pirate|ate   \n",
       "1    uer_gpt2-chinese-cluecorpussmall  en-t0_s0_o0_v1  shark|pirate|helped   \n",
       "2    uer_gpt2-chinese-cluecorpussmall  en-t0_s0_o0_v2  shark|pirate|pushed   \n",
       "3    uer_gpt2-chinese-cluecorpussmall  en-t0_s0_o0_v3  shark|pirate|chased   \n",
       "4    uer_gpt2-chinese-cluecorpussmall  en-t0_s0_o1_v0    shark|student|ate   \n",
       "..                                ...             ...                  ...   \n",
       "195  uer_gpt2-chinese-cluecorpussmall  zh-t0_s4_o3_v3              袋鼠|游客|追   \n",
       "196  uer_gpt2-chinese-cluecorpussmall  zh-t0_s4_o4_v0             袋鼠|政治家|吃   \n",
       "197  uer_gpt2-chinese-cluecorpussmall  zh-t0_s4_o4_v1            袋鼠|政治家|帮助   \n",
       "198  uer_gpt2-chinese-cluecorpussmall  zh-t0_s4_o4_v2             袋鼠|政治家|推   \n",
       "199  uer_gpt2-chinese-cluecorpussmall  zh-t0_s4_o4_v3             袋鼠|政治家|追   \n",
       "\n",
       "    language template_id   subj      obj    verb  \\\n",
       "0         en        en_0  shark   pirate     ate   \n",
       "1         en        en_0  shark   pirate  helped   \n",
       "2         en        en_0  shark   pirate  pushed   \n",
       "3         en        en_0  shark   pirate  chased   \n",
       "4         en        en_0  shark  student     ate   \n",
       "..       ...         ...    ...      ...     ...   \n",
       "195       zh        zh_0     袋鼠       游客       追   \n",
       "196       zh        zh_0     袋鼠      政治家       吃   \n",
       "197       zh        zh_0     袋鼠      政治家      帮助   \n",
       "198       zh        zh_0     袋鼠      政治家       推   \n",
       "199       zh        zh_0     袋鼠      政治家       追   \n",
       "\n",
       "                         sentence  tid  ...  continuation_text_inverse  \\\n",
       "0       A shark ate every pirate.    0  ...    There were many sharks.   \n",
       "1    A shark helped every pirate.    0  ...    There were many sharks.   \n",
       "2    A shark pushed every pirate.    0  ...    There were many sharks.   \n",
       "3    A shark chased every pirate.    0  ...    There were many sharks.   \n",
       "4      A shark ate every student.    0  ...    There were many sharks.   \n",
       "..                            ...  ...  ...                        ...   \n",
       "195                  有一只袋鼠追了每个游客。    0  ...                    有很多只袋鼠。   \n",
       "196                 有一只袋鼠吃了每个政治家。    0  ...                    有很多只袋鼠。   \n",
       "197                有一只袋鼠帮助了每个政治家。    0  ...                    有很多只袋鼠。   \n",
       "198                 有一只袋鼠推了每个政治家。    0  ...                    有很多只袋鼠。   \n",
       "199                 有一只袋鼠追了每个政治家。    0  ...                    有很多只袋鼠。   \n",
       "\n",
       "      continuation_text_surface  \\\n",
       "0     There was only one shark.   \n",
       "1     There was only one shark.   \n",
       "2     There was only one shark.   \n",
       "3     There was only one shark.   \n",
       "4     There was only one shark.   \n",
       "..                          ...   \n",
       "195                     只有一只袋鼠。   \n",
       "196                     只有一只袋鼠。   \n",
       "197                     只有一只袋鼠。   \n",
       "198                     只有一只袋鼠。   \n",
       "199                     只有一只袋鼠。   \n",
       "\n",
       "                                     full_text_inverse  \\\n",
       "0    A shark ate every pirate. There were many sharks.   \n",
       "1    A shark helped every pirate. There were many s...   \n",
       "2    A shark pushed every pirate. There were many s...   \n",
       "3    A shark chased every pirate. There were many s...   \n",
       "4    A shark ate every student. There were many sha...   \n",
       "..                                                 ...   \n",
       "195                               有一只袋鼠追了每个游客。 有很多只袋鼠。   \n",
       "196                              有一只袋鼠吃了每个政治家。 有很多只袋鼠。   \n",
       "197                             有一只袋鼠帮助了每个政治家。 有很多只袋鼠。   \n",
       "198                              有一只袋鼠推了每个政治家。 有很多只袋鼠。   \n",
       "199                              有一只袋鼠追了每个政治家。 有很多只袋鼠。   \n",
       "\n",
       "                                     full_text_surface  delta_sum  ratio_sum  \\\n",
       "0    A shark ate every pirate. There was only one s...   2.095390   8.128613   \n",
       "1    A shark helped every pirate. There was only on...   1.271116   3.564830   \n",
       "2    A shark pushed every pirate. There was only on...   1.041332   2.832989   \n",
       "3    A shark chased every pirate. There was only on...   1.033434   2.810701   \n",
       "4    A shark ate every student. There was only one ...   2.197477   9.002275   \n",
       "..                                                 ...        ...        ...   \n",
       "195                               有一只袋鼠追了每个游客。 只有一只袋鼠。  -7.002993   0.000909   \n",
       "196                              有一只袋鼠吃了每个政治家。 只有一只袋鼠。  -5.853102   0.002871   \n",
       "197                             有一只袋鼠帮助了每个政治家。 只有一只袋鼠。  -5.490833   0.004124   \n",
       "198                              有一只袋鼠推了每个政治家。 只有一只袋鼠。  -6.617328   0.001337   \n",
       "199                              有一只袋鼠追了每个政治家。 只有一只袋鼠。  -6.748816   0.001172   \n",
       "\n",
       "     delta_mean  ratio_mean  preference_sum  preference_mean  \n",
       "0      1.069577    2.914146         inverse          inverse  \n",
       "1      1.008132    2.740476         inverse          inverse  \n",
       "2      0.953306    2.594273         inverse          inverse  \n",
       "3      0.961239    2.614933         inverse          inverse  \n",
       "4      1.089395    2.972474         inverse          inverse  \n",
       "..          ...         ...             ...              ...  \n",
       "195   -1.000427    0.367722         surface          surface  \n",
       "196   -0.836157    0.433373         surface          surface  \n",
       "197   -0.784405    0.456391         surface          surface  \n",
       "198   -0.945333    0.388550         surface          surface  \n",
       "199   -0.964117    0.381320         surface          surface  \n",
       "\n",
       "[200 rows x 30 columns]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scored_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "59f4e4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_long.to_json(\n",
    "    RESULTS_DIR / f\"scored_long_{quantifier_type_str}_{safe_model}.jsonl\",\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    "    force_ascii=False\n",
    ")\n",
    "\n",
    "scored_wide.to_json(\n",
    "    RESULTS_DIR / f\"scored_wide_{quantifier_type_str}_{safe_model}.jsonl\",\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    "    force_ascii=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
