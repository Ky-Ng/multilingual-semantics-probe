{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61b6f226",
   "metadata": {},
   "source": [
    "# Multilingual Semantics Probe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7115d99",
   "metadata": {},
   "source": [
    "## Step 2: Evaluate Log Probs of Continuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7f0a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e639abf",
   "metadata": {},
   "source": [
    "### Load in Continuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f139f006",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSONL_PATH = \"stimuli/stimuli_with_continuations.jsonl\"\n",
    "\n",
    "def read_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                yield json.loads(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f29998aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = list(read_jsonl(JSONL_PATH))\n",
    "continuation_df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c05e25",
   "metadata": {},
   "source": [
    "### Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a52ea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = \"gpt2\"\n",
    "# MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "# MODEL_NAME = \"meta-llama/Llama-3.2-1B\"\n",
    "# MODEL_NAME = \"Qwen/Qwen3-0.6B\"\n",
    "MODEL_NAME = \"google/gemma-3-1b-it\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# DTYPE = torch.float16 if (DEVICE == \"cuda\") else torch.float32\n",
    "DTYPE = torch.float32 # Avoid Precision overflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1889b2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d341895",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_CSV = \"logprob_surface_vs_inverse.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7e0983",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26a9251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|██████████| 2/2 [08:14<00:00, 247.34s/it]\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:31<00:31, 31.48s/it]"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=DTYPE\n",
    ").to(DEVICE)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae5f1d3",
   "metadata": {},
   "source": [
    "### Calculate Continuation Log Prob\n",
    "1) Tokenize Prompt and Continuation\n",
    "\n",
    "2) Calculate Log Probs for prompt + continuation\n",
    "\n",
    "3) Calculate continuation log probs: [a] get log prob of each next token from [0,T-1]; [b] sum log probs\n",
    "- Given a prompt of length T, log_prob[:, :T-1, :] gives the probability of the first [0,T-1] tokens\n",
    "\n",
    "```\n",
    "\n",
    "a shark ate every pirate\n",
    "                    ^ stop here; we already know the probability of this word\n",
    "```\n",
    "\n",
    "```python\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        target_log_probs[b,t] = shifted_log_probs[b, t, target_tokens[b,t]]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba1b175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_probs(prompts: list[str], continuations: list[str]) -> list[dict]:\n",
    "    # 1) Tokenize Prompt and Continuation\n",
    "    enc_base_prompts = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "\n",
    "        # Handle different length prompts\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "\n",
    "        # Avoid [EOS]/[BOS] from being inserted\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "\n",
    "    full_prompts = [p + c for p, c in zip(prompts, continuations)]\n",
    "    enc_full_prompts = tokenizer(\n",
    "        full_prompts,\n",
    "        return_tensors=\"pt\",\n",
    "\n",
    "        # Handle different length prompts\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "\n",
    "        # Avoid [EOS]/[BOS] from being inserted\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "\n",
    "    input_ids = enc_full_prompts[\"input_ids\"].to(DEVICE)\n",
    "    attention_mask = enc_full_prompts[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "    # 2) Calculate logProbs for prompt + continuation\n",
    "    out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = out.logits  # [B, T, V]\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)  # [B, T, V]\n",
    "\n",
    "    # 3) Calculate continuation log probs: [a] get log prob of each next token from [0,T-1]; [b] sum log probs\n",
    "    target_tokens = input_ids[:, 1:]  # [B, T-1]\n",
    "    shifted_log_probs = log_probs[:, :-1, :]  # [B, T-1, V]\n",
    "\n",
    "    # Select the logProb for the selected token\n",
    "    target_log_probs = torch.gather(\n",
    "        input=shifted_log_probs,\n",
    "        dim=-1,  # Select the log_prob for the selected prompt token in vocab\n",
    "        index=target_tokens.unsqueeze(-1)  # [B, T-1, 1]\n",
    "    ).squeeze(-1)  # [B, T-1]\n",
    "\n",
    "    base_prompt_lens = enc_base_prompts[\"attention_mask\"].sum(dim=1).tolist()\n",
    "    full_prompt_lens = enc_full_prompts[\"attention_mask\"].sum(dim=1).tolist()\n",
    "\n",
    "    # The logProbs for the continuation live at (inclusive)\n",
    "    # logits[m-1:n-2] -> logits[m-1:L-2] -> target_log_probs[m-1:L-1] since we already removed one token in the shift\n",
    "    B, _ = target_log_probs.shape\n",
    "\n",
    "    cont_log_probs_list = []\n",
    "\n",
    "    for b in range(B):\n",
    "        base_prompt_length = base_prompt_lens[b]\n",
    "        full_prompt_length = full_prompt_lens[b]\n",
    "\n",
    "        cont_log_probs = target_log_probs[b,\n",
    "                                          base_prompt_length-1:full_prompt_length-1]\n",
    "        cont_log_probs_sum = cont_log_probs.sum().item()\n",
    "        cont_log_probs_mean = cont_log_probs.mean().item()\n",
    "        n_cont_tokens = full_prompt_length - base_prompt_length  # Sanity Check\n",
    "\n",
    "        cont_log_probs_list.append(\n",
    "            {\"cont_log_probs_sum\": cont_log_probs_sum,\n",
    "                \"cont_log_probs_mean\": cont_log_probs_mean,\n",
    "                \"n_cont_tokens\": n_cont_tokens\n",
    "             }\n",
    "        )\n",
    "        \n",
    "    return cont_log_probs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7c57b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_log_probs(df: pd.DataFrame, b_sz: int = BATCH_SIZE):\n",
    "    df = df.reset_index(drop=True).copy() # Pandas safety reasons\n",
    "    all_stats = []\n",
    "\n",
    "    for start in range(0, len(df), b_sz):\n",
    "        end = min(start + b_sz, len(df))\n",
    "        batch = continuation_df.iloc[start:end]\n",
    "\n",
    "        stats = get_log_probs(\n",
    "            prompts=batch[\"sentence\"].tolist(),\n",
    "            continuations=batch[\"continuation_text\"].tolist()\n",
    "        )\n",
    "\n",
    "        all_stats += stats\n",
    "\n",
    "    stats_df = pd.DataFrame(all_stats)    \n",
    "\n",
    "    return pd.concat([df, stats_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dae38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI Generated pivot table magic to get the metrics\n",
    "def collapse_surface_inverse(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df[df[\"continuation_type\"].isin([\"surface\", \"inverse\"])].copy()\n",
    "\n",
    "    num_cols  = [\"cont_log_probs_sum\", \"cont_log_probs_mean\", \"n_cont_tokens\"]\n",
    "    text_cols = [\"continuation_text\", \"full_text\"]\n",
    "\n",
    "    # metadata that is shared across types\n",
    "    meta_cols = [\n",
    "        c for c in df.columns\n",
    "        if c not in ([\"continuation_type\"] + num_cols + text_cols)\n",
    "    ]\n",
    "    meta = df.groupby(\"stimulus_id\", as_index=False)[meta_cols].first()\n",
    "\n",
    "    # --- 1) pivot numeric columns (keeps float/int dtypes) ---\n",
    "    wide_num = (\n",
    "        df[[\"stimulus_id\", \"continuation_type\"] + num_cols]\n",
    "        .pivot(index=\"stimulus_id\", columns=\"continuation_type\", values=num_cols)\n",
    "    )\n",
    "    wide_num.columns = [f\"{col}_{ctype}\" for col, ctype in wide_num.columns]\n",
    "    wide_num = wide_num.reset_index()\n",
    "\n",
    "    # --- 2) pivot text columns (object dtype is fine here) ---\n",
    "    wide_text = (\n",
    "        df[[\"stimulus_id\", \"continuation_type\"] + text_cols]\n",
    "        .pivot(index=\"stimulus_id\", columns=\"continuation_type\", values=text_cols)\n",
    "    )\n",
    "    wide_text.columns = [f\"{col}_{ctype}\" for col, ctype in wide_text.columns]\n",
    "    wide_text = wide_text.reset_index()\n",
    "\n",
    "    # merge everything\n",
    "    out = meta.merge(wide_num, on=\"stimulus_id\", how=\"left\") \\\n",
    "              .merge(wide_text, on=\"stimulus_id\", how=\"left\")\n",
    "\n",
    "    # deltas + ratios (now safe: numeric dtypes)\n",
    "    out[\"delta_sum\"]  = out[\"cont_log_probs_sum_inverse\"]  - out[\"cont_log_probs_sum_surface\"]\n",
    "    out[\"ratio_sum\"]  = np.exp(out[\"delta_sum\"])\n",
    "    \n",
    "    out[\"delta_mean\"] = out[\"cont_log_probs_mean_inverse\"] - out[\"cont_log_probs_mean_surface\"]\n",
    "    out[\"ratio_mean\"] = np.exp(out[\"delta_mean\"])\n",
    "\n",
    "    out[\"preference_mean\"] = np.select(\n",
    "        [out[\"delta_mean\"] > 0, out[\"delta_mean\"] < 0],\n",
    "        [\"inverse\", \"surface\"],\n",
    "        default=\"tie\"\n",
    "    )\n",
    "    out[\"preference_sum\"] = np.select(\n",
    "        [out[\"delta_sum\"] > 0, out[\"delta_sum\"] < 0],\n",
    "        [\"inverse\", \"surface\"],\n",
    "        default=\"tie\"\n",
    "    )\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d694d592",
   "metadata": {},
   "source": [
    "### Save Outputs\n",
    "- Add model name as a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cda37e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "RESULTS_DIR = Path(\"./results\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e51e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "safe_model = re.sub(r\"[^\\w\\-\\.]\", \"_\", MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d6b81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "scored_long = process_log_probs(continuation_df, b_sz=BATCH_SIZE)\n",
    "scored_long.insert(0,\"model\", safe_model)\n",
    "\n",
    "scored_wide = collapse_surface_inverse(scored_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64c6e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>stimulus_id</th>\n",
       "      <th>concept_id</th>\n",
       "      <th>language</th>\n",
       "      <th>template_id</th>\n",
       "      <th>subj</th>\n",
       "      <th>obj</th>\n",
       "      <th>verb</th>\n",
       "      <th>sentence</th>\n",
       "      <th>cont_log_probs_sum_inverse</th>\n",
       "      <th>...</th>\n",
       "      <th>continuation_text_inverse</th>\n",
       "      <th>continuation_text_surface</th>\n",
       "      <th>full_text_inverse</th>\n",
       "      <th>full_text_surface</th>\n",
       "      <th>delta_sum</th>\n",
       "      <th>delta_mean</th>\n",
       "      <th>ratio_sum</th>\n",
       "      <th>ratio_mean</th>\n",
       "      <th>preference_mean</th>\n",
       "      <th>preference_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google_gemma-3-1b-it</td>\n",
       "      <td>en-en_0-000000</td>\n",
       "      <td>shark|pirate|ate</td>\n",
       "      <td>en</td>\n",
       "      <td>en_0</td>\n",
       "      <td>shark</td>\n",
       "      <td>pirate</td>\n",
       "      <td>ate</td>\n",
       "      <td>A shark ate every pirate.</td>\n",
       "      <td>-27.478607</td>\n",
       "      <td>...</td>\n",
       "      <td>There were many sharks.</td>\n",
       "      <td>There was only one shark.</td>\n",
       "      <td>A shark ate every pirate. There were many sharks.</td>\n",
       "      <td>A shark ate every pirate. There was only one s...</td>\n",
       "      <td>2.323645</td>\n",
       "      <td>-0.528679</td>\n",
       "      <td>10.212829</td>\n",
       "      <td>0.589383</td>\n",
       "      <td>surface</td>\n",
       "      <td>inverse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>google_gemma-3-1b-it</td>\n",
       "      <td>en-en_0-000001</td>\n",
       "      <td>shark|pirate|helped</td>\n",
       "      <td>en</td>\n",
       "      <td>en_0</td>\n",
       "      <td>shark</td>\n",
       "      <td>pirate</td>\n",
       "      <td>helped</td>\n",
       "      <td>A shark helped every pirate.</td>\n",
       "      <td>-25.750093</td>\n",
       "      <td>...</td>\n",
       "      <td>There were many sharks.</td>\n",
       "      <td>There was only one shark.</td>\n",
       "      <td>A shark helped every pirate. There were many s...</td>\n",
       "      <td>A shark helped every pirate. There was only on...</td>\n",
       "      <td>2.552305</td>\n",
       "      <td>-0.432952</td>\n",
       "      <td>12.836661</td>\n",
       "      <td>0.648591</td>\n",
       "      <td>surface</td>\n",
       "      <td>inverse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>google_gemma-3-1b-it</td>\n",
       "      <td>en-en_0-000002</td>\n",
       "      <td>shark|pirate|pushed</td>\n",
       "      <td>en</td>\n",
       "      <td>en_0</td>\n",
       "      <td>shark</td>\n",
       "      <td>pirate</td>\n",
       "      <td>pushed</td>\n",
       "      <td>A shark pushed every pirate.</td>\n",
       "      <td>-22.141607</td>\n",
       "      <td>...</td>\n",
       "      <td>There were many sharks.</td>\n",
       "      <td>There was only one shark.</td>\n",
       "      <td>A shark pushed every pirate. There were many s...</td>\n",
       "      <td>A shark pushed every pirate. There was only on...</td>\n",
       "      <td>6.287125</td>\n",
       "      <td>0.309801</td>\n",
       "      <td>537.605292</td>\n",
       "      <td>1.363153</td>\n",
       "      <td>inverse</td>\n",
       "      <td>inverse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>google_gemma-3-1b-it</td>\n",
       "      <td>en-en_0-000003</td>\n",
       "      <td>shark|pirate|chased</td>\n",
       "      <td>en</td>\n",
       "      <td>en_0</td>\n",
       "      <td>shark</td>\n",
       "      <td>pirate</td>\n",
       "      <td>chased</td>\n",
       "      <td>A shark chased every pirate.</td>\n",
       "      <td>-28.556351</td>\n",
       "      <td>...</td>\n",
       "      <td>There were many sharks.</td>\n",
       "      <td>There was only one shark.</td>\n",
       "      <td>A shark chased every pirate. There were many s...</td>\n",
       "      <td>A shark chased every pirate. There was only on...</td>\n",
       "      <td>1.695395</td>\n",
       "      <td>-0.669313</td>\n",
       "      <td>5.448795</td>\n",
       "      <td>0.512060</td>\n",
       "      <td>surface</td>\n",
       "      <td>inverse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>google_gemma-3-1b-it</td>\n",
       "      <td>en-en_0-000004</td>\n",
       "      <td>shark|student|ate</td>\n",
       "      <td>en</td>\n",
       "      <td>en_0</td>\n",
       "      <td>shark</td>\n",
       "      <td>student</td>\n",
       "      <td>ate</td>\n",
       "      <td>A shark ate every student.</td>\n",
       "      <td>-23.287512</td>\n",
       "      <td>...</td>\n",
       "      <td>There were many sharks.</td>\n",
       "      <td>There was only one shark.</td>\n",
       "      <td>A shark ate every student. There were many sha...</td>\n",
       "      <td>A shark ate every student. There was only one ...</td>\n",
       "      <td>6.519796</td>\n",
       "      <td>0.310382</td>\n",
       "      <td>678.440221</td>\n",
       "      <td>1.363947</td>\n",
       "      <td>inverse</td>\n",
       "      <td>inverse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>google_gemma-3-1b-it</td>\n",
       "      <td>zh-zh_0-000123</td>\n",
       "      <td>狗|医生|追</td>\n",
       "      <td>zh</td>\n",
       "      <td>zh_0</td>\n",
       "      <td>狗</td>\n",
       "      <td>医生</td>\n",
       "      <td>追</td>\n",
       "      <td>有一只狗追了每个医生。</td>\n",
       "      <td>-27.165693</td>\n",
       "      <td>...</td>\n",
       "      <td>有很多只狗。</td>\n",
       "      <td>只有一只狗。</td>\n",
       "      <td>有一只狗追了每个医生。 有很多只狗。</td>\n",
       "      <td>有一只狗追了每个医生。 只有一只狗。</td>\n",
       "      <td>-3.175335</td>\n",
       "      <td>-0.635067</td>\n",
       "      <td>0.041780</td>\n",
       "      <td>0.529900</td>\n",
       "      <td>surface</td>\n",
       "      <td>surface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>google_gemma-3-1b-it</td>\n",
       "      <td>zh-zh_0-000124</td>\n",
       "      <td>狗|游客|吃</td>\n",
       "      <td>zh</td>\n",
       "      <td>zh_0</td>\n",
       "      <td>狗</td>\n",
       "      <td>游客</td>\n",
       "      <td>吃</td>\n",
       "      <td>有一只狗吃了每个游客。</td>\n",
       "      <td>-29.891842</td>\n",
       "      <td>...</td>\n",
       "      <td>有很多只狗。</td>\n",
       "      <td>只有一只狗。</td>\n",
       "      <td>有一只狗吃了每个游客。 有很多只狗。</td>\n",
       "      <td>有一只狗吃了每个游客。 只有一只狗。</td>\n",
       "      <td>2.408596</td>\n",
       "      <td>0.481719</td>\n",
       "      <td>11.118340</td>\n",
       "      <td>1.618856</td>\n",
       "      <td>inverse</td>\n",
       "      <td>inverse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>google_gemma-3-1b-it</td>\n",
       "      <td>zh-zh_0-000125</td>\n",
       "      <td>狗|游客|帮助</td>\n",
       "      <td>zh</td>\n",
       "      <td>zh_0</td>\n",
       "      <td>狗</td>\n",
       "      <td>游客</td>\n",
       "      <td>帮助</td>\n",
       "      <td>有一只狗帮助了每个游客。</td>\n",
       "      <td>-28.112892</td>\n",
       "      <td>...</td>\n",
       "      <td>有很多只狗。</td>\n",
       "      <td>只有一只狗。</td>\n",
       "      <td>有一只狗帮助了每个游客。 有很多只狗。</td>\n",
       "      <td>有一只狗帮助了每个游客。 只有一只狗。</td>\n",
       "      <td>2.223234</td>\n",
       "      <td>0.444647</td>\n",
       "      <td>9.237157</td>\n",
       "      <td>1.559939</td>\n",
       "      <td>inverse</td>\n",
       "      <td>inverse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>google_gemma-3-1b-it</td>\n",
       "      <td>zh-zh_0-000126</td>\n",
       "      <td>狗|游客|推</td>\n",
       "      <td>zh</td>\n",
       "      <td>zh_0</td>\n",
       "      <td>狗</td>\n",
       "      <td>游客</td>\n",
       "      <td>推</td>\n",
       "      <td>有一只狗推了每个游客。</td>\n",
       "      <td>-30.290436</td>\n",
       "      <td>...</td>\n",
       "      <td>有很多只狗。</td>\n",
       "      <td>只有一只狗。</td>\n",
       "      <td>有一只狗推了每个游客。 有很多只狗。</td>\n",
       "      <td>有一只狗推了每个游客。 只有一只狗。</td>\n",
       "      <td>0.486448</td>\n",
       "      <td>0.097290</td>\n",
       "      <td>1.626529</td>\n",
       "      <td>1.102179</td>\n",
       "      <td>inverse</td>\n",
       "      <td>inverse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>google_gemma-3-1b-it</td>\n",
       "      <td>zh-zh_0-000127</td>\n",
       "      <td>狗|游客|追</td>\n",
       "      <td>zh</td>\n",
       "      <td>zh_0</td>\n",
       "      <td>狗</td>\n",
       "      <td>游客</td>\n",
       "      <td>追</td>\n",
       "      <td>有一只狗追了每个游客。</td>\n",
       "      <td>-26.698803</td>\n",
       "      <td>...</td>\n",
       "      <td>有很多只狗。</td>\n",
       "      <td>只有一只狗。</td>\n",
       "      <td>有一只狗追了每个游客。 有很多只狗。</td>\n",
       "      <td>有一只狗追了每个游客。 只有一只狗。</td>\n",
       "      <td>-1.879360</td>\n",
       "      <td>-0.375872</td>\n",
       "      <td>0.152688</td>\n",
       "      <td>0.686690</td>\n",
       "      <td>surface</td>\n",
       "      <td>surface</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model     stimulus_id           concept_id language  \\\n",
       "0    google_gemma-3-1b-it  en-en_0-000000     shark|pirate|ate       en   \n",
       "1    google_gemma-3-1b-it  en-en_0-000001  shark|pirate|helped       en   \n",
       "2    google_gemma-3-1b-it  en-en_0-000002  shark|pirate|pushed       en   \n",
       "3    google_gemma-3-1b-it  en-en_0-000003  shark|pirate|chased       en   \n",
       "4    google_gemma-3-1b-it  en-en_0-000004    shark|student|ate       en   \n",
       "..                    ...             ...                  ...      ...   \n",
       "123  google_gemma-3-1b-it  zh-zh_0-000123               狗|医生|追       zh   \n",
       "124  google_gemma-3-1b-it  zh-zh_0-000124               狗|游客|吃       zh   \n",
       "125  google_gemma-3-1b-it  zh-zh_0-000125              狗|游客|帮助       zh   \n",
       "126  google_gemma-3-1b-it  zh-zh_0-000126               狗|游客|推       zh   \n",
       "127  google_gemma-3-1b-it  zh-zh_0-000127               狗|游客|追       zh   \n",
       "\n",
       "    template_id   subj      obj    verb                      sentence  \\\n",
       "0          en_0  shark   pirate     ate     A shark ate every pirate.   \n",
       "1          en_0  shark   pirate  helped  A shark helped every pirate.   \n",
       "2          en_0  shark   pirate  pushed  A shark pushed every pirate.   \n",
       "3          en_0  shark   pirate  chased  A shark chased every pirate.   \n",
       "4          en_0  shark  student     ate    A shark ate every student.   \n",
       "..          ...    ...      ...     ...                           ...   \n",
       "123        zh_0      狗       医生       追                   有一只狗追了每个医生。   \n",
       "124        zh_0      狗       游客       吃                   有一只狗吃了每个游客。   \n",
       "125        zh_0      狗       游客      帮助                  有一只狗帮助了每个游客。   \n",
       "126        zh_0      狗       游客       推                   有一只狗推了每个游客。   \n",
       "127        zh_0      狗       游客       追                   有一只狗追了每个游客。   \n",
       "\n",
       "     cont_log_probs_sum_inverse  ...  continuation_text_inverse  \\\n",
       "0                    -27.478607  ...    There were many sharks.   \n",
       "1                    -25.750093  ...    There were many sharks.   \n",
       "2                    -22.141607  ...    There were many sharks.   \n",
       "3                    -28.556351  ...    There were many sharks.   \n",
       "4                    -23.287512  ...    There were many sharks.   \n",
       "..                          ...  ...                        ...   \n",
       "123                  -27.165693  ...                     有很多只狗。   \n",
       "124                  -29.891842  ...                     有很多只狗。   \n",
       "125                  -28.112892  ...                     有很多只狗。   \n",
       "126                  -30.290436  ...                     有很多只狗。   \n",
       "127                  -26.698803  ...                     有很多只狗。   \n",
       "\n",
       "      continuation_text_surface  \\\n",
       "0     There was only one shark.   \n",
       "1     There was only one shark.   \n",
       "2     There was only one shark.   \n",
       "3     There was only one shark.   \n",
       "4     There was only one shark.   \n",
       "..                          ...   \n",
       "123                      只有一只狗。   \n",
       "124                      只有一只狗。   \n",
       "125                      只有一只狗。   \n",
       "126                      只有一只狗。   \n",
       "127                      只有一只狗。   \n",
       "\n",
       "                                     full_text_inverse  \\\n",
       "0    A shark ate every pirate. There were many sharks.   \n",
       "1    A shark helped every pirate. There were many s...   \n",
       "2    A shark pushed every pirate. There were many s...   \n",
       "3    A shark chased every pirate. There were many s...   \n",
       "4    A shark ate every student. There were many sha...   \n",
       "..                                                 ...   \n",
       "123                                 有一只狗追了每个医生。 有很多只狗。   \n",
       "124                                 有一只狗吃了每个游客。 有很多只狗。   \n",
       "125                                有一只狗帮助了每个游客。 有很多只狗。   \n",
       "126                                 有一只狗推了每个游客。 有很多只狗。   \n",
       "127                                 有一只狗追了每个游客。 有很多只狗。   \n",
       "\n",
       "                                     full_text_surface  delta_sum delta_mean  \\\n",
       "0    A shark ate every pirate. There was only one s...   2.323645  -0.528679   \n",
       "1    A shark helped every pirate. There was only on...   2.552305  -0.432952   \n",
       "2    A shark pushed every pirate. There was only on...   6.287125   0.309801   \n",
       "3    A shark chased every pirate. There was only on...   1.695395  -0.669313   \n",
       "4    A shark ate every student. There was only one ...   6.519796   0.310382   \n",
       "..                                                 ...        ...        ...   \n",
       "123                                 有一只狗追了每个医生。 只有一只狗。  -3.175335  -0.635067   \n",
       "124                                 有一只狗吃了每个游客。 只有一只狗。   2.408596   0.481719   \n",
       "125                                有一只狗帮助了每个游客。 只有一只狗。   2.223234   0.444647   \n",
       "126                                 有一只狗推了每个游客。 只有一只狗。   0.486448   0.097290   \n",
       "127                                 有一只狗追了每个游客。 只有一只狗。  -1.879360  -0.375872   \n",
       "\n",
       "      ratio_sum ratio_mean preference_mean  preference_sum  \n",
       "0     10.212829   0.589383         surface         inverse  \n",
       "1     12.836661   0.648591         surface         inverse  \n",
       "2    537.605292   1.363153         inverse         inverse  \n",
       "3      5.448795   0.512060         surface         inverse  \n",
       "4    678.440221   1.363947         inverse         inverse  \n",
       "..          ...        ...             ...             ...  \n",
       "123    0.041780   0.529900         surface         surface  \n",
       "124   11.118340   1.618856         inverse         inverse  \n",
       "125    9.237157   1.559939         inverse         inverse  \n",
       "126    1.626529   1.102179         inverse         inverse  \n",
       "127    0.152688   0.686690         surface         surface  \n",
       "\n",
       "[128 rows x 25 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scored_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f4e4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_long.to_json(\n",
    "    RESULTS_DIR / f\"scored_long_{safe_model}.jsonl\",\n",
    "    orient=\"records\",\n",
    "    lines=True\n",
    ")\n",
    "\n",
    "scored_wide.to_json(\n",
    "    RESULTS_DIR / f\"scored_wide_{safe_model}.jsonl\",\n",
    "    orient=\"records\",\n",
    "    lines=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
